{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c409bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold comparison analysis...\n",
      "Warning: 10 folds results/test_evaluation/classification_report.txt not found\n",
      "\n",
      "Results saved to: fold_comparison_results.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def parse_classification_report(file_path):\n",
    "    \"\"\"Parse classification_report.txt file and extract key metrics\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Extract accuracy\n",
    "    accuracy_match = re.search(r'accuracy\\s+(\\d+\\.\\d+)', content)\n",
    "    accuracy = float(accuracy_match.group(1)) if accuracy_match else None\n",
    "    \n",
    "    # Extract macro avg precision, recall, and f1-score\n",
    "    macro_avg_match = re.search(r'macro avg\\s+(\\d+\\.\\d+)\\s+(\\d+\\.\\d+)\\s+(\\d+\\.\\d+)', content)\n",
    "    macro_precision = float(macro_avg_match.group(1)) if macro_avg_match else None\n",
    "    macro_recall = float(macro_avg_match.group(2)) if macro_avg_match else None\n",
    "    macro_f1 = float(macro_avg_match.group(3)) if macro_avg_match else None\n",
    "    \n",
    "    return accuracy, macro_precision, macro_recall, macro_f1\n",
    "\n",
    "\n",
    "def compare_folds(base_path=\".\", model_name=\"YourModel\"):\n",
    "    \"\"\"Compare results across all folds\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Process each fold\n",
    "    for fold_num in range(1, 11):  # fold_1 to fold_10\n",
    "        fold_dir = Path(base_path) / f\"fold_{fold_num}\"\n",
    "        report_file = fold_dir / \"nodes_report.txt\"\n",
    "        \n",
    "        if report_file.exists():\n",
    "            try:\n",
    "                accuracy, macro_precision, macro_recall, macro_f1 = parse_classification_report(report_file)\n",
    "\n",
    "                \n",
    "                results.append({\n",
    "                    'model_name': model_name,\n",
    "                    'fold_number': fold_num,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision_macro_avg': macro_precision,\n",
    "                    'recall_macro_avg': macro_recall,\n",
    "                    'f1_score_macro_avg': macro_f1\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing fold_{fold_num}: {e}\")\n",
    "        else:\n",
    "            print(f\"Warning: {report_file} not found\")\n",
    "    \n",
    "    # Process test_evaluation folder\n",
    "    test_dir = Path(base_path) / \"test_evaluation\"\n",
    "    test_report_file = test_dir / \"classification_report.txt\"\n",
    "    \n",
    "    if test_report_file.exists():\n",
    "        try:\n",
    "            accuracy, macro_precision, macro_recall, macro_f1 = parse_classification_report(test_report_file)\n",
    "            \n",
    "            results.append({\n",
    "                'model_name': model_name,\n",
    "                'fold_number': 'Test',\n",
    "                'accuracy': accuracy,\n",
    "                'precision_macro_avg': macro_precision,\n",
    "                'recall_macro_avg': macro_recall,\n",
    "                'f1_score_macro_avg': macro_f1\n",
    "            })\n",
    "            \n",
    "            print(f\"Processed test_evaluation: Accuracy={accuracy:.4f}, Precision={macro_precision:.4f}, Recall={macro_recall:.4f}, Macro F1={macro_f1:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing test_evaluation: {e}\")\n",
    "    else:\n",
    "        print(f\"Warning: {test_report_file} not found\")\n",
    "\n",
    "    \n",
    "    return results\n",
    "\n",
    "def generate_comparison_report(results, model_name):\n",
    "    \"\"\"Generate comparison report and statistics\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to analyze\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Separate test results from fold results\n",
    "    fold_results = df[df['fold_number'] != 'Test']\n",
    "    test_results = df[df['fold_number'] == 'Test']\n",
    "    \n",
    "    # Calculate averages and add average row (only from fold results)\n",
    "    if len(fold_results) > 0:\n",
    "        avg_accuracy = fold_results['accuracy'].mean()\n",
    "        avg_precision = fold_results['precision_macro_avg'].mean()\n",
    "        avg_recall = fold_results['recall_macro_avg'].mean()\n",
    "        avg_f1_macro = fold_results['f1_score_macro_avg'].mean()\n",
    "        \n",
    "        avg_row = {\n",
    "            'model_name': model_name,\n",
    "            'fold_number': 'Avg',\n",
    "            'accuracy': avg_accuracy,\n",
    "            'precision_macro_avg': avg_precision,\n",
    "            'recall_macro_avg': avg_recall,\n",
    "            'f1_score_macro_avg': avg_f1_macro\n",
    "        }\n",
    "        \n",
    "        # Reconstruct dataframe: folds + avg + test\n",
    "        df = pd.concat([\n",
    "            fold_results, \n",
    "            pd.DataFrame([avg_row]), \n",
    "            test_results\n",
    "        ], ignore_index=True)\n",
    "\n",
    "\n",
    "    # Save results to CSV\n",
    "    output_file = \"fold_comparison_results.csv\"\n",
    "    df.to_csv(output_file, index=False, float_format='%.4f')\n",
    "    print(f\"\\nResults saved to: {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    BASE_PATH = \"./10 folds results\"  # Current directory, change if needed\n",
    "    MODEL_NAME = \"Joint RGCN\"  # Change to your actual model name\n",
    "    \n",
    "    print(\"Starting fold comparison analysis...\")\n",
    "    \n",
    "    # Parse all fold results\n",
    "    results = compare_folds(BASE_PATH, MODEL_NAME)\n",
    "    \n",
    "    if results:\n",
    "        # Generate comparison report\n",
    "        df = generate_comparison_report(results, MODEL_NAME)\n",
    "\n",
    "    else:\n",
    "        print(\"No valid results found. Please check your file paths and formats.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fccc12e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading graph data...\n",
      "Loaded 40 graph files.\n",
      "\n",
      "=== Dataset Statistics ===\n",
      "Edge Counts:\n",
      "  Support: 2291\n",
      "  Attack: 145\n",
      "  No Relation: 3016\n",
      "\n",
      "Node Counts:\n",
      "  Premise: 2393\n",
      "  Conclusion: 161\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to the directory containing the graph data files\n",
    "GRAPH_DATA_DIR = Path(\"graph_data_processed_for_joint_prediction\")\n",
    "\n",
    "# Edge type mapping\n",
    "EDGE_TYPE_MAP = {0: \"Support\", 1: \"Attack\", 2: \"No Relation\"}\n",
    "NODE_TYPE_MAP = {0: \"Premise\", 1: \"Conclusion\"}\n",
    "\n",
    "def load_graph_data(graph_data_dir):\n",
    "    \"\"\"Load all preprocessed graph data files.\"\"\"\n",
    "    all_files = sorted([f for f in graph_data_dir.glob(\"*.pt\") if f.is_file()])\n",
    "    if not all_files:\n",
    "        raise FileNotFoundError(f\"No files found in {graph_data_dir}\")\n",
    "    return [torch.load(f, weights_only=False) for f in all_files]\n",
    "\n",
    "def calculate_counts(data_list):\n",
    "    \"\"\"Count edge types and node types in the dataset.\"\"\"\n",
    "    edge_counts = torch.zeros(len(EDGE_TYPE_MAP), dtype=torch.long)\n",
    "    node_counts = torch.zeros(len(NODE_TYPE_MAP), dtype=torch.long)\n",
    "\n",
    "    for data in data_list:\n",
    "        # Count edges\n",
    "        edge_counts += torch.bincount(data.edge_type, minlength=len(EDGE_TYPE_MAP))\n",
    "\n",
    "        # Count nodes\n",
    "        node_counts += torch.bincount(data.y, minlength=len(NODE_TYPE_MAP))\n",
    "\n",
    "    return edge_counts, node_counts\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading graph data...\")\n",
    "    all_data = load_graph_data(GRAPH_DATA_DIR)\n",
    "    print(f\"Loaded {len(all_data)} graph files.\")\n",
    "\n",
    "    # Calculate counts\n",
    "    edge_counts, node_counts = calculate_counts(all_data)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n=== Dataset Statistics ===\")\n",
    "    print(\"Edge Counts:\")\n",
    "    for edge_type, count in enumerate(edge_counts):\n",
    "        print(f\"  {EDGE_TYPE_MAP[edge_type]}: {count}\")\n",
    "\n",
    "    print(\"\\nNode Counts:\")\n",
    "    for node_type, count in enumerate(node_counts):\n",
    "        print(f\"  {NODE_TYPE_MAP[node_type]}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f17a5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 40 graphs.\n",
      "Total Edge Counts: {'Support': 2291, 'Attack': 145, 'No Relation': 3016}\n",
      "Total Node Counts: {'Premise': 2393, 'Conclusion': 161}\n",
      "\n",
      "Fold 1 Statistics:\n",
      "Edge Counts: [265  29 410]\n",
      "Node Counts: [291  17]\n",
      "\n",
      "Fold 2 Statistics:\n",
      "Edge Counts: [431  24 551]\n",
      "Node Counts: [452  30]\n",
      "\n",
      "Fold 3 Statistics:\n",
      "Edge Counts: [169   9 214]\n",
      "Node Counts: [172  12]\n",
      "\n",
      "Fold 4 Statistics:\n",
      "Edge Counts: [111   2 121]\n",
      "Node Counts: [120  11]\n",
      "\n",
      "Fold 5 Statistics:\n",
      "Edge Counts: [372  22 482]\n",
      "Node Counts: [361  25]\n",
      "\n",
      "Fold 6 Statistics:\n",
      "Edge Counts: [388  34 558]\n",
      "Node Counts: [443  29]\n",
      "\n",
      "Fold 7 Statistics:\n",
      "Edge Counts: [335  19 430]\n",
      "Node Counts: [345  24]\n",
      "\n",
      "Fold 8 Statistics:\n",
      "Edge Counts: [48  0 48]\n",
      "Node Counts: [39  2]\n",
      "\n",
      "Fold 9 Statistics:\n",
      "Edge Counts: [ 99   4 119]\n",
      "Node Counts: [101   7]\n",
      "\n",
      "Fold 10 Statistics:\n",
      "Edge Counts: [73  2 83]\n",
      "Node Counts: [69  4]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 109\u001b[39m\n\u001b[32m    106\u001b[39m         \u001b[38;5;66;03m# Train your model here (train_data, val_data)\u001b[39;00m\n\u001b[32m    107\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m graphs, validating on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(val_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m graphs.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[43mrun_joint_rgcn_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 101\u001b[39m, in \u001b[36mrun_joint_rgcn_cross_validation\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     98\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNode Counts:\u001b[39m\u001b[33m\"\u001b[39m, node_counts.sum(axis=\u001b[32m0\u001b[39m))\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Placeholder: Replace this with your RGCN training loop\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m fold_idx, (train_fold, val_fold) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(folds):\n\u001b[32m    102\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_FOLDS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    103\u001b[39m     train_data = [data_list[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m train_fold]\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from pathlib import Path\n",
    "from torch_geometric.data import Data  # Ensure this is imported\n",
    "\n",
    "\n",
    "# Constants\n",
    "GRAPH_DATA_DIR = Path(\"graph_data_processed_for_joint_prediction\")\n",
    "NUM_FOLDS = 10\n",
    "SEED = 42\n",
    "\n",
    "EDGE_TYPE_MAP = {0: \"Support\", 1: \"Attack\", 2: \"No Relation\"}\n",
    "NODE_TYPE_MAP = {0: \"Premise\", 1: \"Conclusion\"}\n",
    "\n",
    "def load_graph_data(graph_data_dir):\n",
    "    \"\"\"Load all graph data files with safe deserialization.\"\"\"\n",
    "    files = sorted([f for f in graph_data_dir.glob(\"*.pt\") if f.is_file()])\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No graph data files found in {graph_data_dir}\")\n",
    "\n",
    "    # Allowlist the Data class for safe unpickling\n",
    "    torch.serialization.add_safe_globals([Data])\n",
    "\n",
    "    return [torch.load(f, weights_only=False) for f in files]\n",
    "\n",
    "\n",
    "def calculate_graph_counts(data_list):\n",
    "    \"\"\"Calculate edge and node type counts for each graph.\"\"\"\n",
    "    edge_counts = []\n",
    "    node_counts = []\n",
    "    for data in data_list:\n",
    "        # Edge counts\n",
    "        edges = torch.bincount(data.edge_type, minlength=len(EDGE_TYPE_MAP))\n",
    "        edge_counts.append(edges.numpy())\n",
    "        # Node counts\n",
    "        nodes = torch.bincount(data.y, minlength=len(NODE_TYPE_MAP))\n",
    "        node_counts.append(nodes.numpy())\n",
    "    return np.array(edge_counts), np.array(node_counts)\n",
    "\n",
    "def create_balanced_splits(data_list, num_folds, seed):\n",
    "    \"\"\"Custom splitting to balance edge and node counts across folds.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    edge_counts, node_counts = calculate_graph_counts(data_list)\n",
    "\n",
    "    # Aggregate counts\n",
    "    total_edge_counts = edge_counts.sum(axis=0)\n",
    "    total_node_counts = node_counts.sum(axis=0)\n",
    "\n",
    "    print(\"Total Edge Counts:\", dict(zip(EDGE_TYPE_MAP.values(), total_edge_counts)))\n",
    "    print(\"Total Node Counts:\", dict(zip(NODE_TYPE_MAP.values(), total_node_counts)))\n",
    "\n",
    "    # Initialize folds\n",
    "    folds = [[] for _ in range(num_folds)]\n",
    "    fold_edge_counts = np.zeros((num_folds, len(EDGE_TYPE_MAP)))\n",
    "    fold_node_counts = np.zeros((num_folds, len(NODE_TYPE_MAP)))\n",
    "\n",
    "    # Shuffle data indices\n",
    "    indices = np.arange(len(data_list))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Distribute graphs into folds\n",
    "    for idx in indices:\n",
    "        graph_edges = edge_counts[idx]\n",
    "        graph_nodes = node_counts[idx]\n",
    "        # Find the fold that increases balance the least\n",
    "        scores = []\n",
    "        for fold in range(num_folds):\n",
    "            updated_edge_counts = fold_edge_counts[fold] + graph_edges\n",
    "            updated_node_counts = fold_node_counts[fold] + graph_nodes\n",
    "            # Calculate imbalance\n",
    "            edge_imbalance = np.std(updated_edge_counts / total_edge_counts)\n",
    "            node_imbalance = np.std(updated_node_counts / total_node_counts)\n",
    "            scores.append(edge_imbalance + node_imbalance)\n",
    "        # Assign graph to the best fold\n",
    "        best_fold = np.argmin(scores)\n",
    "        folds[best_fold].append(idx)\n",
    "        fold_edge_counts[best_fold] += graph_edges\n",
    "        fold_node_counts[best_fold] += graph_nodes\n",
    "\n",
    "    return folds\n",
    "\n",
    "def run_joint_rgcn_cross_validation():\n",
    "    \"\"\"Main function to run cross-validation for the joint RGCN model.\"\"\"\n",
    "    # Load data\n",
    "    data_list = load_graph_data(GRAPH_DATA_DIR)\n",
    "    print(f\"Loaded {len(data_list)} graphs.\")\n",
    "\n",
    "    # Create balanced splits\n",
    "    folds = create_balanced_splits(data_list, NUM_FOLDS, SEED)\n",
    "\n",
    "    # Print fold statistics\n",
    "    for fold_idx, fold in enumerate(folds):\n",
    "        print(f\"\\nFold {fold_idx + 1} Statistics:\")\n",
    "        fold_data = [data_list[idx] for idx in fold]\n",
    "        edge_counts, node_counts = calculate_graph_counts(fold_data)\n",
    "        print(\"Edge Counts:\", edge_counts.sum(axis=0))\n",
    "        print(\"Node Counts:\", node_counts.sum(axis=0))\n",
    "\n",
    "    # Placeholder: Replace this with your RGCN training loop\n",
    "    for fold_idx, (train_fold, val_fold) in enumerate(folds):\n",
    "        print(f\"\\n=== Fold {fold_idx + 1}/{NUM_FOLDS} ===\")\n",
    "        train_data = [data_list[idx] for idx in train_fold]\n",
    "        val_data = [data_list[idx] for idx in val_fold]\n",
    "\n",
    "        # Train your model here (train_data, val_data)\n",
    "        print(f\"Training on {len(train_data)} graphs, validating on {len(val_data)} graphs.\")\n",
    "\n",
    "run_joint_rgcn_cross_validation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae27aa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import RGCNConv\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Constants\n",
    "GRAPH_DATA_DIR = Path(\"graph_data_processed_for_joint_prediction\")\n",
    "NUM_FOLDS = 10\n",
    "SEED = 42\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 5e-3\n",
    "DROPOUT_RATE = 0.5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "EDGE_TYPE_MAP = {0: \"Support\", 1: \"Attack\", 2: \"No Relation\"}\n",
    "NODE_TYPE_MAP = {0: \"Premise\", 1: \"Conclusion\"}\n",
    "\n",
    "# Helper Functions (from previous code)\n",
    "def load_graph_data(graph_data_dir):\n",
    "    \"\"\"Load graph data with secure deserialization.\"\"\"\n",
    "    files = sorted([f for f in graph_data_dir.glob(\"*.pt\") if f.is_file()])\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No graph data files found in {graph_data_dir}\")\n",
    "    from torch_geometric.data import Data\n",
    "    torch.serialization.add_safe_globals([Data])\n",
    "    return [torch.load(f, weights_only=False) for f in files]\n",
    "\n",
    "def calculate_graph_counts(data_list):\n",
    "    \"\"\"Calculate edge and node type counts for each graph.\"\"\"\n",
    "    edge_counts, node_counts = [], []\n",
    "    for data in data_list:\n",
    "        edge_counts.append(torch.bincount(data.edge_type, minlength=len(EDGE_TYPE_MAP)).numpy())\n",
    "        node_counts.append(torch.bincount(data.y, minlength=len(NODE_TYPE_MAP)).numpy())\n",
    "    return np.array(edge_counts), np.array(node_counts)\n",
    "\n",
    "def create_balanced_splits(data_list, num_folds, seed):\n",
    "    \"\"\"Create custom splits to balance edge and node types.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    edge_counts, node_counts = calculate_graph_counts(data_list)\n",
    "\n",
    "    total_edge_counts = edge_counts.sum(axis=0)\n",
    "    total_node_counts = node_counts.sum(axis=0)\n",
    "\n",
    "    folds = [[] for _ in range(num_folds)]\n",
    "    fold_edge_counts = np.zeros((num_folds, len(EDGE_TYPE_MAP)))\n",
    "    fold_node_counts = np.zeros((num_folds, len(NODE_TYPE_MAP)))\n",
    "\n",
    "    indices = np.arange(len(data_list))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    for idx in indices:\n",
    "        graph_edges, graph_nodes = edge_counts[idx], node_counts[idx]\n",
    "        scores = []\n",
    "        for fold in range(num_folds):\n",
    "            updated_edges = fold_edge_counts[fold] + graph_edges\n",
    "            updated_nodes = fold_node_counts[fold] + graph_nodes\n",
    "            edge_imbalance = np.std(updated_edges / total_edge_counts)\n",
    "            node_imbalance = np.std(updated_nodes / total_node_counts)\n",
    "            scores.append(edge_imbalance + node_imbalance)\n",
    "        best_fold = np.argmin(scores)\n",
    "        folds[best_fold].append(idx)\n",
    "        fold_edge_counts[best_fold] += graph_edges\n",
    "        fold_node_counts[best_fold] += graph_nodes\n",
    "\n",
    "    return folds\n",
    "class MultiTaskLoss(torch.nn.Module):\n",
    "    def __init__(self, edge_weight=None, node_weight=None, alpha=0.6):\n",
    "        super().__init__()\n",
    "        self.edge_criterion = torch.nn.CrossEntropyLoss(weight=edge_weight)\n",
    "        self.node_criterion = torch.nn.CrossEntropyLoss(weight=node_weight)\n",
    "        self.alpha = alpha  # Weight for edge vs. node loss\n",
    "\n",
    "    def forward(self, edge_pred, edge_true, node_pred, node_true):\n",
    "        edge_loss = self.edge_criterion(edge_pred, edge_true)\n",
    "        node_loss = self.node_criterion(node_pred, node_true)\n",
    "        return self.alpha * edge_loss + (1 - self.alpha) * node_loss\n",
    "\n",
    "\n",
    "# RGCN Model\n",
    "class EnhancedLegalRGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_relations, dropout=DROPOUT_RATE):\n",
    "        super().__init__()\n",
    "        self.conv1 = RGCNConv(in_channels, hidden_channels, num_relations)\n",
    "        self.conv2 = RGCNConv(hidden_channels, hidden_channels, num_relations)\n",
    "        self.conv3 = RGCNConv(hidden_channels, hidden_channels, num_relations)\n",
    "        self.edge_classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2 * hidden_channels, hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(hidden_channels, len(EDGE_TYPE_MAP))\n",
    "        )\n",
    "        self.node_classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_channels, hidden_channels // 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(hidden_channels // 2, len(NODE_TYPE_MAP))\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_type):\n",
    "        x1 = F.relu(self.conv1(x, edge_index, edge_type))\n",
    "        x2 = F.relu(self.conv2(x1, edge_index, edge_type))\n",
    "        x3 = self.conv3(x2, edge_index, edge_type)\n",
    "\n",
    "        row, col = edge_index\n",
    "        edge_features = torch.cat([x3[row], x3[col]], dim=-1)\n",
    "        edge_out = self.edge_classifier(edge_features)\n",
    "        node_out = self.node_classifier(x3)\n",
    "\n",
    "        return edge_out, node_out\n",
    "\n",
    "# Training and Evaluation\n",
    "def train_epoch(model, data_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in data_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        edge_pred, node_pred = model(data.x, data.edge_index, data.edge_type)\n",
    "        loss = criterion(edge_pred, data.edge_type, node_pred, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "def validate(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    edge_true, edge_pred = [], []\n",
    "    node_true, node_pred = [], []\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            data = data.to(device)\n",
    "            edge_out, node_out = model(data.x, data.edge_index, data.edge_type)\n",
    "            loss = criterion(edge_out, data.edge_type, node_out, data.y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            edge_true.extend(data.edge_type.cpu().numpy())\n",
    "            edge_pred.extend(edge_out.argmax(dim=1).cpu().numpy())\n",
    "            node_true.extend(data.y.cpu().numpy())\n",
    "            node_pred.extend(node_out.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "    edge_f1 = f1_score(edge_true, edge_pred, average=\"macro\")\n",
    "    node_f1 = f1_score(node_true, node_pred, average=\"macro\")\n",
    "    return total_loss / len(data_loader), edge_f1, node_f1\n",
    "\n",
    "def run_joint_rgcn_cross_validation():\n",
    "    data_list = load_graph_data(GRAPH_DATA_DIR)\n",
    "    folds = create_balanced_splits(data_list, NUM_FOLDS, SEED)\n",
    "    all_f1_scores = []\n",
    "\n",
    "    for fold_idx, val_indices in enumerate(folds):\n",
    "        print(f\"=== Fold {fold_idx + 1}/{NUM_FOLDS} ===\")\n",
    "        train_indices = [i for i in range(len(data_list)) if i not in val_indices]\n",
    "        train_data = [data_list[idx] for idx in train_indices]\n",
    "        val_data = [data_list[idx] for idx in val_indices]\n",
    "\n",
    "        # Initialize Model\n",
    "        model = EnhancedLegalRGCN(in_channels=770, hidden_channels=32, num_relations=3).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=3)\n",
    "\n",
    "        # Compute class weights for balanced training\n",
    "        edge_weight = calculate_class_weights(train_data, \"edge_type\", len(EDGE_TYPE_MAP))\n",
    "        node_weight = calculate_class_weights(train_data, \"y\", len(NODE_TYPE_MAP))\n",
    "        criterion = MultiTaskLoss(edge_weight=edge_weight, node_weight=node_weight)\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "        early_stop_counter = 0\n",
    "        max_patience = 5\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            train_loss = train_epoch(model, train_data, optimizer, criterion)\n",
    "            val_loss, edge_f1, node_f1 = validate(model, val_data, criterion)\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{EPOCHS} | Train Loss: {train_loss:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} | Edge F1: {edge_f1:.4f} | Node F1: {node_f1:.4f}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                early_stop_counter = 0\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "                if early_stop_counter >= max_patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "        # Save F1 scores for fold\n",
    "        all_f1_scores.append((edge_f1, node_f1))\n",
    "\n",
    "    avg_edge_f1 = np.mean([score[0] for score in all_f1_scores])\n",
    "    avg_node_f1 = np.mean([score[1] for score in all_f1_scores])\n",
    "    print(f\"\\nFinal Average Edge F1: {avg_edge_f1:.4f}\")\n",
    "    print(f\"Final Average Node F1: {avg_node_f1:.4f}\")\n",
    "\n",
    "def calculate_class_weights(data_list, attr, num_classes):\n",
    "    counts = torch.zeros(num_classes, device=device)  # Ensure counts is on the correct device\n",
    "    for data in data_list:\n",
    "        attr_tensor = getattr(data, attr).to(device)  # Move the attribute tensor to the same device\n",
    "        counts += torch.bincount(attr_tensor, minlength=num_classes)\n",
    "    weights = 1.0 / (counts + 1e-5)  # Avoid division by zero\n",
    "    return weights\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_joint_rgcn_cross_validation()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "argmining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
