{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01443a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suyamoon/miniconda3/lib/python3.12/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "/home/suyamoon/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-16 22:40:10.714717: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-16 22:40:10.836319: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744823410.894686 2684402 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744823410.912598 2684402 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744823411.023066 2684402 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744823411.023109 2684402 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744823411.023111 2684402 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744823411.023113 2684402 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-16 22:40:11.036611: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "030c93e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "CLASS_WEIGHTS = {\n",
    "    'relation': torch.tensor([0.05, 0.3, 0.65]),  # Adjust based on your class distribution\n",
    "    'source': torch.tensor([0.1, 0.2, 0.7]),      # non-arg, premise, conclusion\n",
    "    'target': torch.tensor([0.1, 0.2, 0.7])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143f15a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28bd4a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgumentDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.max_len = max_len\n",
    "        self.label_map = {\n",
    "            'relation': {'no-relation': 0, 'support': 1, 'attack': 2},\n",
    "            'source_type': {'non-argumentative': 0, 'premise': 1, 'conclusion': 2},\n",
    "            'target_type': {'non-argumentative': 0, 'premise': 1, 'conclusion': 2}\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        \n",
    "        # Tokenize text pair\n",
    "        text = row['source_text'] + \" </s></s> \" + row['target_text']\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        \n",
    "        # Convert labels\n",
    "        return {\n",
    "            'input_ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n",
    "            'relation_label': torch.tensor(self.label_map['relation'][row['relation']], dtype=torch.long),\n",
    "            'source_label': torch.tensor(self.label_map['source_type'][row['source_type']], dtype=torch.long),\n",
    "            'target_label': torch.tensor(self.label_map['target_type'][row['target_type']], dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a327e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskRoberta(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiTaskRoberta, self).__init__()\n",
    "        self.roberta = RobertaForSequenceClassification.from_pretrained(\"roberta-base\").roberta\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        \n",
    "        # Task-specific heads\n",
    "        self.relation_classifier = torch.nn.Linear(768, 3)\n",
    "        self.source_classifier = torch.nn.Linear(768, 3)\n",
    "        self.target_classifier = torch.nn.Linear(768, 3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        return (\n",
    "            self.relation_classifier(pooled_output),\n",
    "            self.source_classifier(pooled_output),\n",
    "            self.target_classifier(pooled_output)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84680c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('balanced_relations_dataset.csv')\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['relation'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0c07bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and datasets\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "train_dataset = ArgumentDataset(train_df, tokenizer, MAX_LEN)\n",
    "val_dataset = ArgumentDataset(val_df, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "161cd746",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4945f76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suyamoon/miniconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/suyamoon/miniconda3/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MultiTaskRoberta().to(device)\n",
    "\n",
    "optimizer = AdamW([\n",
    "    {'params': model.roberta.parameters(), 'lr': 1e-5},\n",
    "    {'params': model.relation_classifier.parameters(), 'lr': 2e-4},\n",
    "    {'params': model.source_classifier.parameters(), 'lr': 2e-4},\n",
    "    {'params': model.target_classifier.parameters(), 'lr': 2e-4}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e533896",
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_criterion = torch.nn.CrossEntropyLoss(weight=CLASS_WEIGHTS['relation'].to(device))\n",
    "source_criterion = torch.nn.CrossEntropyLoss(weight=CLASS_WEIGHTS['source'].to(device))\n",
    "target_criterion = torch.nn.CrossEntropyLoss(weight=CLASS_WEIGHTS['target'].to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46f15201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot and save training curves\n",
    "def plot_training_curves(history, save_dir, current_epoch):\n",
    "    \"\"\"Plot and save training curves\"\"\"\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Plot 1: Loss curves\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(epochs, history['train_loss'], 'b-', label='Training Loss')\n",
    "    plt.plot(epochs, history['val_loss'], 'r-', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot 2: Relation F1 scores\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(epochs, history['relation_f1'], 'g-', label='Macro F1')\n",
    "    plt.plot(epochs, history['relation_f1_classes']['no-relation'], 'c--', label='No-relation')\n",
    "    plt.plot(epochs, history['relation_f1_classes']['support'], 'm--', label='Support')\n",
    "    plt.plot(epochs, history['relation_f1_classes']['attack'], 'y--', label='Attack')\n",
    "    plt.title('Relation Classification F1 Scores')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot 3: Source Type Accuracy\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(epochs, history['source_acc'], 'b-', label='Source Accuracy')\n",
    "    plt.title('Source Type Classification Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot 4: Target Type Accuracy\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(epochs, history['target_acc'], 'r-', label='Target Accuracy')\n",
    "    plt.title('Target Type Classification Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(os.path.join(save_dir, f'training_curves_epoch_{current_epoch+1}.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Save metrics as CSV\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'epoch': epochs,\n",
    "        'train_loss': history['train_loss'],\n",
    "        'val_loss': history['val_loss'],\n",
    "        'relation_f1': history['relation_f1'],\n",
    "        'relation_f1_no_relation': history['relation_f1_classes']['no-relation'],\n",
    "        'relation_f1_support': history['relation_f1_classes']['support'],\n",
    "        'relation_f1_attack': history['relation_f1_classes']['attack'],\n",
    "        'source_acc': history['source_acc'],\n",
    "        'target_acc': history['target_acc']\n",
    "    })\n",
    "    metrics_df.to_csv(os.path.join(save_dir, 'training_metrics.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e838da13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Epoch 1/3\n",
      "========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training:  36%|███▌      | 87/245 [11:22<20:40,  7.85s/batch, loss=0.8792, avg_loss=0.8445, batch_time=8.28s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 49\u001b[0m\n\u001b[1;32m     44\u001b[0m loss \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0.6\u001b[39m \u001b[38;5;241m*\u001b[39m relation_criterion(outputs[\u001b[38;5;241m0\u001b[39m], inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelation_label\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;241m0.2\u001b[39m \u001b[38;5;241m*\u001b[39m source_criterion(outputs[\u001b[38;5;241m1\u001b[39m], inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_label\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;241m0.2\u001b[39m \u001b[38;5;241m*\u001b[39m target_criterion(outputs[\u001b[38;5;241m2\u001b[39m], inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_label\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Update metrics\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create results directory\n",
    "results_dir = f\"results_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Initialize tracking variables\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'relation_f1': [],\n",
    "    'source_acc': [],\n",
    "    'target_acc': [],\n",
    "    'relation_f1_classes': {'no-relation': [], 'support': [], 'attack': []},\n",
    "    'source_acc_classes': {'non-arg': [], 'premise': [], 'conclusion': []},\n",
    "    'target_acc_classes': {'non-arg': [], 'premise': [], 'conclusion': []}\n",
    "}\n",
    "\n",
    "# Training Loop with Enhanced Logging and Checkpointing\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"{'='*40}\\n\")\n",
    "    \n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    batch_time = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize progress bar for training\n",
    "    train_pbar = tqdm(enumerate(train_loader), \n",
    "                     total=len(train_loader),\n",
    "                     desc=f\"Epoch {epoch+1} Training\",\n",
    "                     unit=\"batch\")\n",
    "    \n",
    "    for batch_idx, batch in train_pbar:\n",
    "        batch_start = time.time()\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'text'}\n",
    "        outputs = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = (0.6 * relation_criterion(outputs[0], inputs['relation_label']) +\n",
    "                0.2 * source_criterion(outputs[1], inputs['source_label']) +\n",
    "                0.2 * target_criterion(outputs[2], inputs['target_label']))\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update metrics\n",
    "        total_loss += loss.item()\n",
    "        batch_time += time.time() - batch_start\n",
    "        \n",
    "        # Update progress bar\n",
    "        train_pbar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'avg_loss': f\"{total_loss/(batch_idx+1):.4f}\",\n",
    "            'batch_time': f\"{time.time()-batch_start:.2f}s\"\n",
    "        })\n",
    "    \n",
    "    # Calculate epoch metrics\n",
    "    epoch_time = time.time() - start_time\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_preds = {'relation': [], 'source': [], 'target': []}\n",
    "    all_labels = {'relation': [], 'source': [], 'target': []}\n",
    "    \n",
    "    # Initialize progress bar for validation\n",
    "    val_pbar = tqdm(enumerate(val_loader), \n",
    "                   total=len(val_loader),\n",
    "                   desc=f\"Epoch {epoch+1} Validation\",\n",
    "                   unit=\"batch\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in val_pbar:\n",
    "            batch_start = time.time()\n",
    "            \n",
    "            # Forward pass\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'text'}\n",
    "            outputs = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = (0.6 * relation_criterion(outputs[0], inputs['relation_label']) +\n",
    "                    0.2 * source_criterion(outputs[1], inputs['source_label']) +\n",
    "                    0.2 * target_criterion(outputs[2], inputs['target_label']))\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Store predictions\n",
    "            for i, task in enumerate(['relation', 'source', 'target']):\n",
    "                preds = torch.argmax(outputs[i], dim=1).cpu().numpy()\n",
    "                labels = inputs[f'{task}_label'].cpu().numpy()\n",
    "                all_preds[task].extend(preds)\n",
    "                all_labels[task].extend(labels)\n",
    "            \n",
    "            # Update progress bar\n",
    "            val_pbar.set_postfix({\n",
    "                'val_loss': f\"{loss.item():.4f}\",\n",
    "                'avg_val_loss': f\"{val_loss/(batch_idx+1):.4f}\",\n",
    "                'batch_time': f\"{time.time()-batch_start:.2f}s\"\n",
    "            })\n",
    "    \n",
    "    # Calculate validation metrics\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    history['val_loss'].append(avg_val_loss)\n",
    "    \n",
    "    # Detailed Classification Reports\n",
    "    relation_report = classification_report(\n",
    "        all_labels['relation'], all_preds['relation'],\n",
    "        target_names=['no-relation', 'support', 'attack'],\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    source_report = classification_report(\n",
    "        all_labels['source'], all_preds['source'],\n",
    "        target_names=['non-arg', 'premise', 'conclusion'],\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    target_report = classification_report(\n",
    "        all_labels['target'], all_preds['target'],\n",
    "        target_names=['non-arg', 'premise', 'conclusion'],\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Time: {epoch_time:.2f}s ({batch_time/len(train_loader):.2f}s/batch)\")\n",
    "    \n",
    "    print(\"\\nRelation Classification Report:\")\n",
    "    print(relation_report)\n",
    "    \n",
    "    print(\"Source Type Classification Report:\")\n",
    "    print(source_report)\n",
    "    \n",
    "    print(\"Target Type Classification Report:\")\n",
    "    print(target_report)\n",
    "    \n",
    "    print(f\"{'='*40}\\n\")\n",
    "    \n",
    "    history['relation_f1'].append(relation_report['macro avg']['f1-score'])\n",
    "    history['relation_f1_classes']['no-relation'].append(relation_report['no-relation']['f1-score'])\n",
    "    history['relation_f1_classes']['support'].append(relation_report['support']['f1-score'])\n",
    "    history['relation_f1_classes']['attack'].append(relation_report['attack']['f1-score'])\n",
    "    \n",
    "    # Store accuracies\n",
    "    history['source_acc'].append(source_report['accuracy'])\n",
    "    history['target_acc'].append(target_report['accuracy'])\n",
    "    \n",
    "    # Save model checkpoint\n",
    "    checkpoint_path = os.path.join(results_dir, f\"checkpoint_epoch_{epoch+1}.pt\")\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': avg_train_loss,\n",
    "        'val_loss': avg_val_loss\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "    \n",
    "    plot_training_curves(history, results_dir, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec6862a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bdda9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'roberta_multi_task_model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
