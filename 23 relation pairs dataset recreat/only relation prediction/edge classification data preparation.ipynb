{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eb13362",
   "metadata": {},
   "source": [
    "# graph data preparation"
   ]
  },
  {
   "cell_type": "code",
<<<<<<<< HEAD:23 relation pairs dataset recreat/edge classification graph data preparation.ipynb
   "execution_count": 12,
========
   "execution_count": 3,
>>>>>>>> 9ddd4f9 (included .pt files from A40):23 relation pairs dataset recreat/only relation prediction/edge classification data preparation.ipynb
   "id": "8dc077bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
<<<<<<<< HEAD:23 relation pairs dataset recreat/edge classification graph data preparation.ipynb
   "execution_count": 13,
========
   "execution_count": 4,
>>>>>>>> 9ddd4f9 (included .pt files from A40):23 relation pairs dataset recreat/only relation prediction/edge classification data preparation.ipynb
   "id": "7d8c5277",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = Path(\"3 PC_SANR_final.csv\")\n",
    "MODEL_DIR = Path(\"best_model_legalbert_pc\") #suyamoonpathak/legalbert-prem-conc-finetuned\n",
<<<<<<<< HEAD:23 relation pairs dataset recreat/edge classification graph data preparation.ipynb
    "OUTPUT_DIR = Path(\"LegalBERT_PC_processed_graph_data_for_edge_prediction_csv\")\n",
========
    "OUTPUT_DIR = Path(\"LegalBERT_processed_graph_data_for_edge_prediction_csv\")\n",
>>>>>>>> 9ddd4f9 (included .pt files from A40):23 relation pairs dataset recreat/only relation prediction/edge classification data preparation.ipynb
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<<< HEAD:23 relation pairs dataset recreat/edge classification graph data preparation.ipynb
   "execution_count": 14,
========
   "execution_count": 5,
>>>>>>>> 9ddd4f9 (included .pt files from A40):23 relation pairs dataset recreat/only relation prediction/edge classification data preparation.ipynb
   "id": "a9ef9223",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('suyamoonpathak/legalbert-prem-conc-finetuned')\n",
    "model = AutoModel.from_pretrained('suyamoonpathak/legalbert-prem-conc-finetuned')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<<< HEAD:23 relation pairs dataset recreat/edge classification graph data preparation.ipynb
   "execution_count": 15,
========
   "execution_count": 6,
>>>>>>>> 9ddd4f9 (included .pt files from A40):23 relation pairs dataset recreat/only relation prediction/edge classification data preparation.ipynb
   "id": "6f885954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge type mapping from CSV relations to integers compatible with previous code convention\n",
    "relation_to_edge_type = {\n",
    "    \"support\": 0,\n",
    "    \"attack\": 1,\n",
    "    \"no-relation\": 2\n",
    "}\n",
    "\n",
    "# Node label mapping for node classification\n",
    "node_type_to_label = {\n",
    "    \"prem\": 0,\n",
    "    \"conc\": 1,\n",
    "}\n",
    "\n",
    "# Priority order for node type when a node appears as both source and target with different types\n",
    "node_type_priority = {\n",
    "    \"conc\": 3,\n",
    "    \"prem\": 2,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<<< HEAD:23 relation pairs dataset recreat/edge classification graph data preparation.ipynb
   "execution_count": 16,
========
   "execution_count": 7,
>>>>>>>> 9ddd4f9 (included .pt files from A40):23 relation pairs dataset recreat/only relation prediction/edge classification data preparation.ipynb
   "id": "2c66deeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(texts, batch_size=4):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch,\n",
    "                           padding=True,\n",
    "                           truncation=True,\n",
    "                           max_length=512,\n",
    "                           return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state[:, 0, :].cpu())\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "def generate_raw_embeddings_from_word_embeddings(texts, batch_size=8):\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch, \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=512, \n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        \n",
    "        # Extract token IDs from the tokenizer output\n",
    "        input_ids = inputs['input_ids']\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get the embeddings directly from the embedding layer\n",
    "            word_embeddings = model.embeddings.word_embeddings(input_ids)\n",
    "            \n",
    "            cls_embeddings = word_embeddings[:, 0, :]  \n",
    "            \n",
    "            embeddings.append(cls_embeddings.cpu())\n",
    "    \n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "\n",
    "def generate_gaussian_embeddings(texts, batch_size=4):\n",
    "    hidden_size = model.config.hidden_size\n",
    "    # hidden_size should be set to match model.config.hidden_size\n",
    "    if hidden_size is None:\n",
    "        raise ValueError(\"hidden_size must be specified to match the model's output dimensionality.\")\n",
    "    \n",
    "    embeddings = []\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch_len = len(texts[i:i+batch_size])\n",
    "        # Generate gaussian noise with shape [batch_len, hidden_size]\n",
    "        noise = torch.randn(batch_len, hidden_size).to(device)\n",
    "        embeddings.append(noise.cpu())\n",
    "    \n",
    "    return torch.cat(embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<<< HEAD:23 relation pairs dataset recreat/edge classification graph data preparation.ipynb
   "execution_count": 17,
========
   "execution_count": 8,
>>>>>>>> 9ddd4f9 (included .pt files from A40):23 relation pairs dataset recreat/only relation prediction/edge classification data preparation.ipynb
   "id": "20ac44ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_node_types(source_infos, target_infos):\n",
    "    \"\"\"\n",
    "    Given dictionaries mapping text to set of source_types and target_types,\n",
    "    determine final node type by priority:\n",
    "    conclusion > premise >\n",
    "    \"\"\"\n",
    "    node_types = {}\n",
    "    all_nodes = set(list(source_infos.keys()) + list(target_infos.keys()))\n",
    "    \n",
    "    for text in all_nodes:\n",
    "        source_types = source_infos.get(text, set())\n",
    "        target_types = target_infos.get(text, set())\n",
    "        combined_types = source_types.union(target_types)\n",
    "        \n",
    "        # Pick type by priority order\n",
    "        best_type = None\n",
    "        best_priority = 0\n",
    "        for t in combined_types:\n",
    "            prio = node_type_priority.get(t, 0)\n",
    "            if prio > best_priority:\n",
    "                best_priority = prio\n",
    "                best_type = t\n",
    "        # if best_type is None:\n",
    "        #     # fallback to non-argumentative if something unexpected\n",
    "        #     best_type = \"non-argumentative\"\n",
    "        node_types[text] = best_type\n",
    "    return node_types\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<<< HEAD:23 relation pairs dataset recreat/edge classification graph data preparation.ipynb
   "execution_count": 18,
========
   "execution_count": 9,
>>>>>>>> 9ddd4f9 (included .pt files from A40):23 relation pairs dataset recreat/only relation prediction/edge classification data preparation.ipynb
   "id": "e16bf9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "file_names = df['file_name'].unique()\n",
    "assert len(file_names) == 40, f\"Expected 40 files, found {len(file_names)}\"\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<<< HEAD:23 relation pairs dataset recreat/edge classification graph data preparation.ipynb
   "execution_count": 19,
========
   "execution_count": 10,
>>>>>>>> 9ddd4f9 (included .pt files from A40):23 relation pairs dataset recreat/only relation prediction/edge classification data preparation.ipynb
   "id": "f019fe28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<<< HEAD:23 relation pairs dataset recreat/edge classification graph data preparation.ipynb
      "Processing cases:   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 24/24 [00:06<00:00,  3.94it/s]\n",
      "Generating embeddings: 100%|██████████| 25/25 [00:13<00:00,  1.90it/s]\n",
      "Generating embeddings: 100%|██████████| 14/14 [00:06<00:00,  2.03it/s]\n",
      "Generating embeddings: 100%|██████████| 19/19 [00:10<00:00,  1.88it/s]\n",
      "Generating embeddings: 100%|██████████| 13/13 [00:06<00:00,  2.15it/s]\n",
      "Generating embeddings: 100%|██████████| 24/24 [00:10<00:00,  2.26it/s]\n",
      "Generating embeddings: 100%|██████████| 28/28 [00:13<00:00,  2.01it/s]\n",
      "Generating embeddings: 100%|██████████| 14/14 [00:07<00:00,  1.99it/s]\n",
      "Generating embeddings: 100%|██████████| 19/19 [00:10<00:00,  1.84it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:03<00:00,  2.83it/s]\n",
      "Generating embeddings: 100%|██████████| 8/8 [00:02<00:00,  3.68it/s]\n",
      "Generating embeddings: 100%|██████████| 20/20 [00:04<00:00,  4.51it/s]\n",
      "Generating embeddings: 100%|██████████| 17/17 [00:04<00:00,  3.45it/s]\n",
      "Generating embeddings: 100%|██████████| 18/18 [00:08<00:00,  2.16it/s]\n",
      "Generating embeddings: 100%|██████████| 36/36 [00:11<00:00,  3.17it/s]\n",
      "Generating embeddings: 100%|██████████| 9/9 [00:02<00:00,  3.58it/s]\n",
      "Generating embeddings: 100%|██████████| 13/13 [00:03<00:00,  3.49it/s]\n",
      "Generating embeddings: 100%|██████████| 18/18 [00:05<00:00,  3.38it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:02<00:00,  3.90it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:02<00:00,  3.95it/s]\n",
      "Generating embeddings: 100%|██████████| 7/7 [00:01<00:00,  3.91it/s]\n",
      "Generating embeddings: 100%|██████████| 16/16 [00:05<00:00,  3.09it/s]\n",
      "Generating embeddings: 100%|██████████| 7/7 [00:01<00:00,  3.52it/s]\n",
      "Generating embeddings: 100%|██████████| 20/20 [00:05<00:00,  3.50it/s]\n",
      "Generating embeddings: 100%|██████████| 19/19 [00:04<00:00,  3.98it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:02<00:00,  3.47it/s]\n",
      "Generating embeddings: 100%|██████████| 28/28 [00:06<00:00,  4.04it/s]\n",
      "Generating embeddings: 100%|██████████| 14/14 [00:04<00:00,  3.30it/s]\n",
      "Generating embeddings: 100%|██████████| 12/12 [00:04<00:00,  2.58it/s]\n",
      "Generating embeddings: 100%|██████████| 12/12 [00:03<00:00,  3.63it/s]\n",
      "Generating embeddings: 100%|██████████| 16/16 [00:05<00:00,  3.07it/s]\n",
      "Generating embeddings: 100%|██████████| 12/12 [00:03<00:00,  3.06it/s]\n",
      "Generating embeddings: 100%|██████████| 13/13 [00:03<00:00,  3.65it/s]\n",
      "Generating embeddings: 100%|██████████| 14/14 [00:03<00:00,  3.78it/s]\n",
      "Generating embeddings: 100%|██████████| 13/13 [00:04<00:00,  3.15it/s]\n",
      "Generating embeddings: 100%|██████████| 37/37 [00:10<00:00,  3.41it/s]\n",
      "Generating embeddings: 100%|██████████| 9/9 [00:03<00:00,  2.63it/s]\n",
      "Generating embeddings: 100%|██████████| 19/19 [00:05<00:00,  3.36it/s]\n",
      "Generating embeddings: 100%|██████████| 12/12 [00:03<00:00,  3.15it/s]\n",
      "Generating embeddings: 100%|██████████| 17/17 [00:04<00:00,  3.81it/s]\n",
      "Processing cases: 100%|██████████| 40/40 [03:48<00:00,  5.72s/it]\n"
========
      "Generating embeddings: 100%|██████████| 24/24 [00:01<00:00, 21.51it/s]\n",
      "Generating embeddings: 100%|██████████| 25/25 [00:00<00:00, 74.59it/s]\n",
      "Generating embeddings: 100%|██████████| 14/14 [00:00<00:00, 60.80it/s]\n",
      "Generating embeddings: 100%|██████████| 19/19 [00:00<00:00, 71.39it/s]\n",
      "Generating embeddings: 100%|██████████| 13/13 [00:00<00:00, 81.53it/s]\n",
      "Generating embeddings: 100%|██████████| 24/24 [00:00<00:00, 85.48it/s]\n",
      "Generating embeddings: 100%|██████████| 28/28 [00:00<00:00, 75.19it/s]\n",
      "Generating embeddings: 100%|██████████| 14/14 [00:00<00:00, 91.86it/s]\n",
      "Generating embeddings: 100%|██████████| 19/19 [00:00<00:00, 76.29it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 73.69it/s]\n",
      "Generating embeddings: 100%|██████████| 8/8 [00:00<00:00, 68.22it/s]\n",
      "Generating embeddings: 100%|██████████| 20/20 [00:00<00:00, 72.52it/s]\n",
      "Generating embeddings: 100%|██████████| 17/17 [00:00<00:00, 82.65it/s]\n",
      "Generating embeddings: 100%|██████████| 18/18 [00:00<00:00, 70.38it/s]\n",
      "Generating embeddings: 100%|██████████| 36/36 [00:00<00:00, 85.16it/s]\n",
      "Generating embeddings: 100%|██████████| 9/9 [00:00<00:00, 78.80it/s]\n",
      "Generating embeddings: 100%|██████████| 13/13 [00:00<00:00, 83.73it/s]\n",
      "Generating embeddings: 100%|██████████| 17/17 [00:00<00:00, 70.65it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 70.57it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:00<00:00, 69.35it/s]\n",
      "Generating embeddings: 100%|██████████| 7/7 [00:00<00:00, 61.18it/s]\n",
      "Generating embeddings: 100%|██████████| 16/16 [00:00<00:00, 71.30it/s]\n",
      "Generating embeddings: 100%|██████████| 7/7 [00:00<00:00, 67.55it/s]\n",
      "Generating embeddings: 100%|██████████| 20/20 [00:00<00:00, 74.28it/s]\n",
      "Generating embeddings: 100%|██████████| 18/18 [00:00<00:00, 78.14it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:00<00:00, 73.90it/s]\n",
      "Generating embeddings: 100%|██████████| 28/28 [00:00<00:00, 92.39it/s]\n",
      "Generating embeddings: 100%|██████████| 14/14 [00:00<00:00, 73.79it/s]\n",
      "Generating embeddings: 100%|██████████| 12/12 [00:00<00:00, 72.30it/s]\n",
      "Generating embeddings: 100%|██████████| 12/12 [00:00<00:00, 74.53it/s]\n",
      "Generating embeddings: 100%|██████████| 16/16 [00:00<00:00, 70.91it/s]\n",
      "Generating embeddings: 100%|██████████| 12/12 [00:00<00:00, 69.48it/s]\n",
      "Generating embeddings: 100%|██████████| 13/13 [00:00<00:00, 69.67it/s]\n",
      "Generating embeddings: 100%|██████████| 14/14 [00:00<00:00, 72.08it/s]\n",
      "Generating embeddings: 100%|██████████| 13/13 [00:00<00:00, 77.24it/s]\n",
      "Generating embeddings: 100%|██████████| 37/37 [00:00<00:00, 86.77it/s]\n",
      "Generating embeddings: 100%|██████████| 9/9 [00:00<00:00, 65.57it/s]\n",
      "Generating embeddings: 100%|██████████| 19/19 [00:00<00:00, 78.22it/s]\n",
      "Generating embeddings: 100%|██████████| 12/12 [00:00<00:00, 68.93it/s]\n",
      "Generating embeddings: 100%|██████████| 17/17 [00:00<00:00, 77.02it/s]\n",
      "Processing cases: 100%|██████████| 40/40 [00:13<00:00,  2.89it/s]\n"
>>>>>>>> 9ddd4f9 (included .pt files from A40):23 relation pairs dataset recreat/only relation prediction/edge classification data preparation.ipynb
     ]
    }
   ],
   "source": [
    "all_data = []\n",
    "\n",
    "for file_name in tqdm(file_names, desc=\"Processing cases\"):\n",
    "    sub_df = df[df['file_name'] == file_name]\n",
    "\n",
    "    # 1. Extract all unique node texts (source and target)\n",
    "    source_texts = sub_df['source_text'].tolist()\n",
    "    target_texts = sub_df['target_text'].tolist()\n",
    "    unique_texts = list(set(source_texts).union(set(target_texts)))\n",
    "\n",
    "    # 2. Collect node types from source_type and target_type\n",
    "    # Map text to set of source_types or target_types (because text can appear multiple times)\n",
    "    source_type_map = {}\n",
    "    target_type_map = {}\n",
    "    for _, row in sub_df.iterrows():\n",
    "        # source\n",
    "        st = row['source_text']\n",
    "        s_type = row['source_type'].strip().lower()\n",
    "        source_type_map.setdefault(st, set()).add(s_type)\n",
    "        # target\n",
    "        tt = row['target_text']\n",
    "        t_type = row['target_type'].strip().lower()\n",
    "        target_type_map.setdefault(tt, set()).add(t_type)\n",
    "\n",
    "    # 3. Determine final node types by priority of presence among source and target types\n",
    "    node_types = determine_node_types(source_type_map, target_type_map)\n",
    "\n",
    "    # 4. Map each unique text to index\n",
    "    text_to_idx = {text: idx for idx, text in enumerate(unique_texts)}\n",
    "\n",
    "    # 5. Generate embeddings for nodes\n",
    "    embeddings = generate_embeddings(unique_texts)\n",
    "\n",
    "    # 6. Create one-hot encoded node type features (2 classes)\n",
    "    node_features_type = torch.zeros((len(unique_texts), 2))\n",
    "    node_labels = []\n",
    "    for i, text in enumerate(unique_texts):\n",
    "        ntype = node_types[text]\n",
    "        label = node_type_to_label.get(ntype, 2)  # Default non-argumentative if missing\n",
    "        node_labels.append(label)\n",
    "        if ntype == \"prem\":\n",
    "            node_features_type[i, 0] = 1\n",
    "        elif ntype == \"conc\":\n",
    "            node_features_type[i, 1] = 1\n",
    "        # else:\n",
    "        #     node_features_type[i, 2] = 1\n",
    "\n",
    "    node_labels = torch.tensor(node_labels, dtype=torch.long)\n",
    "    node_features = torch.cat([embeddings, node_features_type], dim=1)\n",
    "\n",
    "    # 7. Build edges - edge indices and edge types\n",
    "    edge_indices = []\n",
    "    edge_types = []\n",
    "    for _, row in sub_df.iterrows():\n",
    "        src_text = row['source_text']\n",
    "        tgt_text = row['target_text']\n",
    "        rel = row['relation'].strip().lower()\n",
    "        if src_text in text_to_idx and tgt_text in text_to_idx:\n",
    "            edge_indices.append([text_to_idx[src_text], text_to_idx[tgt_text]])\n",
    "            edge_types.append(relation_to_edge_type[rel])\n",
    "        else:\n",
    "            print(f\"Warning: Missing node index for edge {src_text} -> {tgt_text} in file {file_name}\")\n",
    "\n",
    "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "    edge_type = torch.tensor(edge_types, dtype=torch.long)\n",
    "\n",
    "    # 8. Create Data object\n",
    "    data = Data(\n",
    "        x=node_features,\n",
    "        edge_index=edge_index,\n",
    "        edge_type=edge_type,\n",
    "        y=node_labels,\n",
    "        xml_file=file_name\n",
    "    )\n",
    "\n",
    "    # 9. Save Data object as .pt file with filename exactly as file_name.pt\n",
    "    output_path = OUTPUT_DIR / f\"{file_name}.pt\"\n",
    "    torch.save(data, output_path)\n",
    "    all_data.append(data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<<< HEAD:23 relation pairs dataset recreat/edge classification graph data preparation.ipynb
   "execution_count": 20,
========
   "execution_count": 11,
>>>>>>>> 9ddd4f9 (included .pt files from A40):23 relation pairs dataset recreat/only relation prediction/edge classification data preparation.ipynb
   "id": "0befaa8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved 40 files.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Processed and saved {len(all_data)} files.\")\n",
    "\n",
    "assert len(all_data) == 40, f\"Warning: processed file count mismatch, expected 40 but got {len(all_data)}\""
   ]
  },
  {
   "cell_type": "code",
<<<<<<<< HEAD:23 relation pairs dataset recreat/edge classification graph data preparation.ipynb
   "execution_count": 21,
========
   "execution_count": 12,
>>>>>>>> 9ddd4f9 (included .pt files from A40):23 relation pairs dataset recreat/only relation prediction/edge classification data preparation.ipynb
   "id": "ebb383fe",
   "metadata": {},
   "outputs": [
    {
<<<<<<<< HEAD:23 relation pairs dataset recreat/edge classification graph data preparation.ipynb
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Dataset Summary\n",
      "- **Total Documents**: 40\n",
      "- **Total Arguments**: 2559\n",
      "- **Support Relationships**: 2272\n",
      "- **Attack Relationships**: 145\n",
      "- **No-Relation Pairs**: 5000\n",
      "- **Ratio (Support:Attack:NoRel)**: 2272:145:5000\n",
      "\n",
      "\n",
      "**A2009_Commission of the European Communities v Koninklijke FrieslandCampina NV_M.xml.pt**\n",
      "- Nodes: 56\n",
      "- Support: 48\n",
      "- Attack: 0\n",
      "- No-Relation: 125\n",
      "\n",
      "**A2016_European Commission v Aer Lingus Ltd and Ryanair Designated Activity Company.xml.pt**\n",
      "- Nodes: 66\n",
      "- Support: 61\n",
      "- Attack: 2\n",
      "- No-Relation: 125\n",
      "\n",
      "**A2012_BNP Paribas and Banca Nazionale del Lavoro SpA (BNL) v European Commission.xml.pt**\n",
      "- Nodes: 61\n",
      "- Support: 28\n",
      "- Attack: 4\n",
      "- No-Relation: 125\n",
      "\n",
      "**R2011_France Télécom SA v European Commission.xml.pt**\n",
      "- Nodes: 94\n",
      "- Support: 77\n",
      "- Attack: 0\n",
      "- No-Relation: 125\n",
      "\n",
      "**A2011_European Commission (C-106_09 P) and Kingdom of Spain (C-107_09 P) v Government of Gibraltar and United Kingdom of Great Britain and Northern Ireland.xml.pt**\n",
      "- Nodes: 50\n",
      "- Support: 46\n",
      "- Attack: 2\n",
      "- No-Relation: 125\n",
      "\n",
      "**R2016_Orange v European Commission.xml.pt**\n",
      "- Nodes: 65\n",
      "- Support: 69\n",
      "- Attack: 9\n",
      "- No-Relation: 125\n",
      "\n",
      "**R2000_French Republic v Ladbroke Racing Ltd and Commission of the European Communitie.xml.pt**\n",
      "- Nodes: 73\n",
      "- Support: 47\n",
      "- Attack: 16\n",
      "- No-Relation: 125\n",
      "\n",
      "**A2008_Commission of the European Communities v Salzgitter AG.xml.pt**\n",
      "- Nodes: 56\n",
      "- Support: 48\n",
      "- Attack: 2\n",
      "- No-Relation: 125\n",
      "\n",
      "**R2013_Telefónica SA v European Commission.xml.pt**\n",
      "- Nodes: 37\n",
      "- Support: 33\n",
      "- Attack: 1\n",
      "- No-Relation: 125\n",
      "\n",
      "**R2016_DTS Distribuidora de Televisión Digital.xml.pt**\n",
      "- Nodes: 96\n",
      "- Support: 84\n",
      "- Attack: 6\n",
      "- No-Relation: 125\n",
      "\n",
      "**A2017_Ellinikos Chrysos AE Metalleion kai Viomichanias Chrysou v European Commission.xml.pt**\n",
      "- Nodes: 33\n",
      "- Support: 24\n",
      "- Attack: 1\n",
      "- No-Relation: 125\n",
      "\n",
      "**A2013_European Commission v Ireland and Others.xml.pt**\n",
      "- Nodes: 46\n",
      "- Support: 47\n",
      "- Attack: 0\n",
      "- No-Relation: 125\n",
      "\n",
      "**R2015_European Commission v MOL Magyar Olaj- és Gázipari Nyrt.xml.pt**\n",
      "- Nodes: 45\n",
      "- Support: 40\n",
      "- Attack: 0\n",
      "- No-Relation: 125\n",
      "\n",
      "**R2004_Ramondín SA and Ramondín Cápsulas SA (C-186_02 P) and Territorio Histórico de Álava - Diputación Foral de Álava (C-188_02 P) v Commission of the European Communities.xml.pt**\n",
      "- Nodes: 31\n",
      "- Support: 30\n",
      "- Attack: 1\n",
      "- No-Relation: 125\n",
      "\n",
      "**R2016_European Commission v Hansestadt Lübeck.xml.pt**\n",
      "- Nodes: 75\n",
      "- Support: 65\n",
      "- Attack: 0\n",
      "- No-Relation: 125\n",
      "\n",
      "**R2006_European Commission v Italian Republic.xml.pt**\n",
      "- Nodes: 41\n",
      "- Support: 38\n",
      "- Attack: 1\n",
      "- No-Relation: 125\n",
      "\n",
      "**A2017_European Commission v Italian Republic_DT.xml.pt**\n",
      "- Nodes: 52\n",
      "- Support: 59\n",
      "- Attack: 1\n",
      "- No-Relation: 125\n",
      "\n",
      "**R2002_associação dos refinadores de açúcar portugueses.xml.pt**\n",
      "- Nodes: 111\n",
      "- Support: 95\n",
      "- Attack: 2\n",
      "- No-Relation: 125\n",
      "\n",
      "**R2012_European Commission v Électricité de France (EDF).xml.pt**\n",
      "- Nodes: 51\n",
      "- Support: 52\n",
      "- Attack: 5\n",
      "- No-Relation: 125\n",
      "\n",
      "**R2021_Prosegur Compañía de Seguridad SA, established in Madrid (Spain) v Commission.xml.pt**\n",
      "- Nodes: 146\n",
      "- Support: 116\n",
      "- Attack: 22\n",
      "- No-Relation: 125\n",
      "\n",
      "**R2017_European Commission v Frucona Košice a.xml.pt**\n",
      "- Nodes: 53\n",
      "- Support: 56\n",
      "- Attack: 0\n",
      "- No-Relation: 125\n",
      "\n",
      "**R2004_Daewoo Electronics Manufacturing España SA and Territorio Histórico de Álava - Diputación Foral de Álava v Commission of the European Communities.xml.pt**\n",
      "- Nodes: 26\n",
      "- Support: 20\n",
      "- Attack: 0\n",
      "- No-Relation: 125\n",
      "\n",
      "**R2021_FVE Holýšov I and Others v Commission.xml.pt**\n",
      "- Nodes: 55\n",
      "- Support: 34\n",
      "- Attack: 8\n",
      "- No-Relation: 125\n",
      "\n",
      "**R2004_Italian Republic v Commission of the European Communities.xml.pt**\n",
      "- Nodes: 79\n",
      "- Support: 67\n",
      "- Attack: 0\n",
      "- No-Relation: 125\n",
      "\n",
      "**A2017_European Commission v TV2_Danmark A_S.xml.pt**\n",
      "- Nodes: 33\n",
      "- Support: 30\n",
      "- Attack: 1\n",
      "- No-Relation: 125\n",
      "\n",
      "**R2011_European Commission v Kronoply GmbH & Co.xml.pt**\n",
      "- Nodes: 41\n",
      "- Support: 48\n",
      "- Attack: 0\n",
      "- No-Relation: 125\n",
      "\n",
      "**A2016_European_Commission_v_World_Duty_Free.xml.pt**\n",
      "- Nodes: 72\n",
      "- Support: 73\n",
      "- Attack: 1\n",
      "- No-Relation: 125\n",
      "\n",
      "**R2010_AceaElectrabel Produzione SpA v European Commission.xml.pt**\n",
      "- Nodes: 69\n",
      "- Support: 65\n",
      "- Attack: 0\n",
      "- No-Relation: 125\n",
      "\n",
      "**R2021_World Duty Free v. Commission.xml.pt**\n",
      "- Nodes: 143\n",
      "- Support: 116\n",
      "- Attack: 22\n",
      "- No-Relation: 125\n",
      "\n",
      "**A2018_Commission v Spain.xml.pt**\n",
      "- Nodes: 97\n",
      "- Support: 75\n",
      "- Attack: 5\n",
      "- No-Relation: 125\n",
      "\n",
      "**A2018_Dirk Andres v European Commission.xml.pt**\n",
      "- Nodes: 73\n",
      "- Support: 76\n",
      "- Attack: 5\n",
      "- No-Relation: 125\n",
      "\n",
      "**R1997_Tiercé Ladbroke SA v Commission of the European Communities.xml.pt**\n",
      "- Nodes: 25\n",
      "- Support: 23\n",
      "- Attack: 4\n",
      "- No-Relation: 125\n",
      "\n",
      "**A2013_Frucona Košice a.s. v European Commission.xml.pt**\n",
      "- Nodes: 46\n",
      "- Support: 43\n",
      "- Attack: 1\n",
      "- No-Relation: 125\n",
      "\n",
      "**R2016_Netherlands Maritime Technology Association formerly Scheepsbouw Nederland v European Commission.xml.pt**\n",
      "- Nodes: 77\n",
      "- Support: 58\n",
      "- Attack: 10\n",
      "- No-Relation: 125\n",
      "\n",
      "**A2010_NDSHT Nya Destination Stockholm Hotell & Teaterpaket AB v European Commission.xml.pt**\n",
      "- Nodes: 38\n",
      "- Support: 39\n",
      "- Attack: 1\n",
      "- No-Relation: 125\n",
      "\n",
      "**R2013_3F, formerly Specialarbejderforbundet i Danmark (SID) v European Commission.xml.pt**\n",
      "- Nodes: 52\n",
      "- Support: 44\n",
      "- Attack: 2\n",
      "- No-Relation: 125\n",
      "\n",
      "**A2018_Scuola Elementare Maria Montessori Srl v European Commission.xml.pt**\n",
      "- Nodes: 112\n",
      "- Support: 107\n",
      "- Attack: 3\n",
      "- No-Relation: 125\n",
      "\n",
      "**A2009_3F v Commission of the European Communities.xml.pt**\n",
      "- Nodes: 63\n",
      "- Support: 59\n",
      "- Attack: 4\n",
      "- No-Relation: 125\n",
      "\n",
      "**R2016_Hellenic Republic v European Commission.xml.pt**\n",
      "- Nodes: 73\n",
      "- Support: 90\n",
      "- Attack: 0\n",
      "- No-Relation: 125\n",
      "\n",
      "**R2017_Viasat Broadcasting UK Ltd v European Commission.xml.pt**\n",
      "- Nodes: 47\n",
      "- Support: 42\n",
      "- Attack: 3\n",
      "- No-Relation: 125\n"
========
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m total_stats\n\u001b[32m     46\u001b[39m stats = get_dataset_stats()\n\u001b[32m     48\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[33m## Dataset Summary\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[33m- **Total Documents**: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43mstats\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfiles\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33m- **Total Arguments**: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats[\u001b[33m'\u001b[39m\u001b[33mtotal_nodes\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[33m- **Support Relationships**: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats[\u001b[33m'\u001b[39m\u001b[33mtotal_support\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[33m- **Attack Relationships**: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats[\u001b[33m'\u001b[39m\u001b[33mtotal_attack\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[33m- **No-Relation Pairs**: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats[\u001b[33m'\u001b[39m\u001b[33mtotal_no_relation\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[33m- **Ratio (Support:Attack:NoRel)**: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats[\u001b[33m'\u001b[39m\u001b[33mtotal_support\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats[\u001b[33m'\u001b[39m\u001b[33mtotal_attack\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats[\u001b[33m'\u001b[39m\u001b[33mtotal_no_relation\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[33m\"\"\"\u001b[39m)\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m stats[\u001b[33m'\u001b[39m\u001b[33mfiles\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m     59\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[33m**\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile[\u001b[33m'\u001b[39m\u001b[33mfilename\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m**\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[33m- Nodes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile[\u001b[33m'\u001b[39m\u001b[33mnodes\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[33m- Support: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile[\u001b[33m'\u001b[39m\u001b[33msupport\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[33m- Attack: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile[\u001b[33m'\u001b[39m\u001b[33mattack\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[33m- No-Relation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile[\u001b[33m'\u001b[39m\u001b[33mno_relation\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\"\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: string indices must be integers, not 'str'"
>>>>>>>> 9ddd4f9 (included .pt files from A40):23 relation pairs dataset recreat/only relation prediction/edge classification data preparation.ipynb
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
<<<<<<<< HEAD:23 relation pairs dataset recreat/edge classification graph data preparation.ipynb
    "def get_dataset_stats(output_dir=\"LegalBERT_PC_raw_graph_data_for_edge_prediction_csv\"):\n",
========
    "def get_dataset_stats(output_dir=\"LegalBERT_raw_graph_data_for_joint_prediction_csv\"):\n",
>>>>>>>> 9ddd4f9 (included .pt files from A40):23 relation pairs dataset recreat/only relation prediction/edge classification data preparation.ipynb
    "    pt_files = list(Path(output_dir).glob(\"*.pt\"))\n",
    "    if not pt_files:\n",
    "        return \"No processed files found. Run XML processing first.\"\n",
    "\n",
    "    total_stats = {\n",
    "        'total_nodes': 0,\n",
    "        'total_support': 0,\n",
    "        'total_attack': 0,\n",
    "        'total_no_relation': 0,  \n",
    "        'files': []\n",
    "    }\n",
    "\n",
    "\n",
    "    for pt_file in pt_files:\n",
    "        try:\n",
    "            data = torch.load(pt_file, weights_only=False)\n",
    "            support = (data.edge_type == 0).sum().item()\n",
    "            attack = (data.edge_type == 1).sum().item()\n",
    "            no_relation = (data.edge_type == 2).sum().item()  \n",
    "\n",
    "            \n",
    "            file_stats = {\n",
    "                'filename': pt_file.name,\n",
    "                'nodes': data.x.shape[0],\n",
    "                'support': support,\n",
    "                'attack': attack,\n",
    "                'no_relation': no_relation\n",
    "            }\n",
    "            \n",
    "            total_stats['total_nodes'] += file_stats['nodes']\n",
    "            total_stats['total_support'] += support\n",
    "            total_stats['total_attack'] += attack\n",
    "            total_stats['total_no_relation'] += no_relation\n",
    "            total_stats['files'].append(file_stats)\n",
    "            \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {pt_file.name}: {str(e)}\")\n",
    "    \n",
    "    return total_stats\n",
    "\n",
    "stats = get_dataset_stats()\n",
    "\n",
    "print(f\"\"\"\n",
    "## Dataset Summary\n",
    "- **Total Documents**: {len(stats['files'])}\n",
    "- **Total Arguments**: {stats['total_nodes']}\n",
    "- **Support Relationships**: {stats['total_support']}\n",
    "- **Attack Relationships**: {stats['total_attack']}\n",
    "- **No-Relation Pairs**: {stats['total_no_relation']}\n",
    "- **Ratio (Support:Attack:NoRel)**: {stats['total_support']}:{stats['total_attack']}:{stats['total_no_relation']}\n",
    "\"\"\")\n",
    "\n",
    "for file in stats['files']:\n",
    "    print(f\"\"\"\n",
    "**{file['filename']}**\n",
    "- Nodes: {file['nodes']}\n",
    "- Support: {file['support']}\n",
    "- Attack: {file['attack']}\n",
    "- No-Relation: {file['no_relation']}\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<<< HEAD:23 relation pairs dataset recreat/edge classification graph data preparation.ipynb
   "execution_count": 22,
========
   "execution_count": null,
>>>>>>>> 9ddd4f9 (included .pt files from A40):23 relation pairs dataset recreat/only relation prediction/edge classification data preparation.ipynb
   "id": "58c1fbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support count: 2272\n",
      "Attack count: 145\n",
      "No-relation count: 5000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the filtered CSV file\n",
    "df = pd.read_csv('3 PC_SANR_final.csv')\n",
    "\n",
    "# Count occurrences of each relation type in the 'relation' column\n",
    "relation_counts = df['relation'].value_counts()\n",
    "\n",
    "# Extract counts for 'support', 'attack', and 'no-relation'\n",
    "support_count = relation_counts.get('support', 0)\n",
    "attack_count = relation_counts.get('attack', 0)\n",
    "no_relation_count = relation_counts.get('no-relation', 0)\n",
    "\n",
    "# Print the counts\n",
    "print(f\"Support count: {support_count}\")\n",
    "print(f\"Attack count: {attack_count}\")\n",
    "print(f\"No-relation count: {no_relation_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ae9602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "argmining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
