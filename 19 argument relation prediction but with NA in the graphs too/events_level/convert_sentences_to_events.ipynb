{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6284e95b",
   "metadata": {},
   "source": [
    "# ucreat events extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cae103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:32: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:34: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:35: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:36: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:38: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:39: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:67: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:317: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:32: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:34: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:35: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:36: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:38: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:39: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:67: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:317: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/tmp/ipykernel_250508/275267298.py:32: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  text = [re.sub(\"\\s+\", \" \", sentence) for sentence in text]\n",
      "/tmp/ipykernel_250508/275267298.py:34: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  text = [re.sub(\"\\.\\.+\", \"\", sentence) for sentence in text]\n",
      "/tmp/ipykernel_250508/275267298.py:35: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  text = [re.sub(\"\\A ?\", \"\", sentence) for sentence in text]\n",
      "/tmp/ipykernel_250508/275267298.py:36: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  text = [sentence for sentence in text if(len(sentence) != 1 and not re.fullmatch(\"(\\d|\\d\\d|\\d\\d\\d)\", sentence))]\n",
      "/tmp/ipykernel_250508/275267298.py:38: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  text = [re.sub('\\A\\(?(\\d|\\d\\d\\d|\\d\\d|[a-zA-Z])(\\.|\\))\\s?(?=[A-Z])', '\\n', sentence) for sentence in text]\n",
      "/tmp/ipykernel_250508/275267298.py:39: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  text = [re.sub(\"\\A\\(([ivx]+)\\)\\s?(?=[a-zA-Z0-9])\", '\\n', sentence) for sentence in text]\n",
      "/tmp/ipykernel_250508/275267298.py:67: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  regex = re.compile(\"[^a-zA-Z<>.\\s]\")\n",
      "/tmp/ipykernel_250508/275267298.py:317: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  regex = re.compile(\"[^a-zA-Z<>.\\s]\")\n",
      "/tmp/ipykernel_250508/275267298.py:32: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  text = [re.sub(\"\\s+\", \" \", sentence) for sentence in text]\n",
      "/tmp/ipykernel_250508/275267298.py:34: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  text = [re.sub(\"\\.\\.+\", \"\", sentence) for sentence in text]\n",
      "/tmp/ipykernel_250508/275267298.py:35: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  text = [re.sub(\"\\A ?\", \"\", sentence) for sentence in text]\n",
      "/tmp/ipykernel_250508/275267298.py:36: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  text = [sentence for sentence in text if(len(sentence) != 1 and not re.fullmatch(\"(\\d|\\d\\d|\\d\\d\\d)\", sentence))]\n",
      "/tmp/ipykernel_250508/275267298.py:38: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  text = [re.sub('\\A\\(?(\\d|\\d\\d\\d|\\d\\d|[a-zA-Z])(\\.|\\))\\s?(?=[A-Z])', '\\n', sentence) for sentence in text]\n",
      "/tmp/ipykernel_250508/275267298.py:39: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  text = [re.sub(\"\\A\\(([ivx]+)\\)\\s?(?=[a-zA-Z0-9])\", '\\n', sentence) for sentence in text]\n",
      "/tmp/ipykernel_250508/275267298.py:67: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  regex = re.compile(\"[^a-zA-Z<>.\\s]\")\n",
      "/tmp/ipykernel_250508/275267298.py:317: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  regex = re.compile(\"[^a-zA-Z<>.\\s]\")\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_trf'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Load spaCy model (assumes GPU is available)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# spacy.require_gpu()\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_core_web_trf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m alphabet_string \u001b[38;5;241m=\u001b[39m string\u001b[38;5;241m.\u001b[39mascii_lowercase\n\u001b[1;32m     13\u001b[0m alphabet_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(alphabet_string)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/spacy/__init__.py:52\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     29\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     36\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/spacy/util.py:484\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 484\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_trf'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import string\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Load spaCy model (assumes GPU is available)\n",
    "spacy.require_gpu()\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "alphabet_string = string.ascii_lowercase\n",
    "alphabet_list = list(alphabet_string)\n",
    "exclusion_list = alphabet_list + [\n",
    "    \"no\", \"nos\", \"sub-s\", \"subs\", \"ss\", \"cl\", \"dr\", \"mr\", \"mrs\", \"dr\", \"vs\", \"ch\", \"addl\",\n",
    "]\n",
    "exclusion_list = [word + \".\" for word in exclusion_list]\n",
    "\n",
    "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
    "OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\", \"pobj\"]\n",
    "ADJECTIVES = [\"acomp\", \"advcl\", \"advmod\", \"amod\", \"appos\", \"nn\", \"nmod\", \"ccomp\", \"complm\", \"hmod\", \"infmod\", \"xcomp\", \"rcmod\", \"poss\", \" possessive\"]\n",
    "ADVERBS = [\"advmod\"]\n",
    "COMPOUNDS = [\"compound\"]\n",
    "PREPOSITIONS = [\"prep\"]\n",
    "\n",
    "def preprocess(content):\n",
    "    raw_text = re.sub(r\"\\xa0\", \" \", str(content))\n",
    "    raw_text = raw_text.split(\"\\n\")\n",
    "    text = raw_text.copy()\n",
    "    text = [re.sub(r'[^a-zA-Z0-9.,<>)\\-(/?\\t ]', '', sentence) for sentence in text]\n",
    "    text = [re.sub(\"\\t+\", \" \", sentence) for sentence in text]\n",
    "    text = [re.sub(\"\\s+\", \" \", sentence) for sentence in text]\n",
    "    text = [re.sub(\" +\", \" \", sentence) for sentence in text]\n",
    "    text = [re.sub(\"\\.\\.+\", \"\", sentence) for sentence in text]\n",
    "    text = [re.sub(\"\\A ?\", \"\", sentence) for sentence in text]\n",
    "    text = [sentence for sentence in text if(len(sentence) != 1 and not re.fullmatch(\"(\\d|\\d\\d|\\d\\d\\d)\", sentence))]\n",
    "    text = [sentence for sentence in text if len(sentence) != 0]\n",
    "    text = [re.sub('\\A\\(?(\\d|\\d\\d\\d|\\d\\d|[a-zA-Z])(\\.|\\))\\s?(?=[A-Z])', '\\n', sentence) for sentence in text]\n",
    "    text = [re.sub(\"\\A\\(([ivx]+)\\)\\s?(?=[a-zA-Z0-9])\", '\\n', sentence) for sentence in text]\n",
    "    text = [re.sub(r\"[()[\\]\\\"$']\", \" \", sentence) for sentence in text]\n",
    "    text = [re.sub(r\" no.\", \" number \", sentence, flags=re.I) for sentence in text]\n",
    "    text = [re.sub(r\" nos.\", \" numbers \", sentence, flags=re.I) for sentence in text]\n",
    "    text = [re.sub(r\" co.\", \" company \", sentence) for sentence in text]\n",
    "    text = [re.sub(r\" ltd.\", \" limited \", sentence, flags=re.I) for sentence in text]\n",
    "    text = [re.sub(r\" pvt.\", \" private \", sentence, flags=re.I) for sentence in text]\n",
    "    text = [re.sub(r\" vs\\.?\", \" versus \", sentence, flags=re.I) for sentence in text]\n",
    "    text = [re.sub(r\"ors\\.?\", \"others\", sentence, flags=re.I) for sentence in text]\n",
    "    text2 = []\n",
    "    for index in range(len(text)):\n",
    "        if(index > 0 and text[index] == '' and text[index-1] == ''):\n",
    "            continue\n",
    "        if(index < len(text)-1 and text[index+1] != '' and text[index+1][0] == '\\n' and text[index] == ''):\n",
    "            continue\n",
    "        text2.append(text[index])\n",
    "    text = text2\n",
    "    text = \"\\n\".join(text)\n",
    "    lines = text.split(\"\\n\")\n",
    "    text_new = \" \".join(lines)\n",
    "    text_new = re.sub(\" +\", \" \", text_new)\n",
    "    l_new = []\n",
    "    for token in text_new.split():\n",
    "        if token.lower() not in exclusion_list:\n",
    "            l_new.append(token.strip())\n",
    "    return \" \".join(l_new)\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    regex = re.compile(\"[^a-zA-Z<>.\\s]\")\n",
    "    text_returned = re.sub(regex, \" \", text)\n",
    "    tokens = text_returned.split()\n",
    "    words = []\n",
    "    for word in tokens:\n",
    "        if len(word) > 1 or word in single_words:\n",
    "            words.append(word)\n",
    "    return \" \".join(words)\n",
    "\n",
    "def getSubsFromConjunctions(subs):\n",
    "    moreSubs = []\n",
    "    for sub in subs:\n",
    "        rights = list(sub.rights)\n",
    "        rightDeps = {tok.lower_ for tok in rights}\n",
    "        if \"and\" in rightDeps:\n",
    "            moreSubs.extend([tok for tok in rights if tok.dep_ in SUBJECTS or tok.pos_ == \"NOUN\"])\n",
    "            if len(moreSubs) > 0:\n",
    "                moreSubs.extend(getSubsFromConjunctions(moreSubs))\n",
    "    return moreSubs\n",
    "\n",
    "def getObjsFromConjunctions(objs):\n",
    "    moreObjs = []\n",
    "    for obj in objs:\n",
    "        rights = list(obj.rights)\n",
    "        rightDeps = {tok.lower_ for tok in rights}\n",
    "        if \"and\" in rightDeps:\n",
    "            moreObjs.extend([tok for tok in rights if tok.dep_ in OBJECTS or tok.pos_ == \"NOUN\"])\n",
    "            if len(moreObjs) > 0:\n",
    "                moreObjs.extend(getObjsFromConjunctions(moreObjs))\n",
    "    return moreObjs\n",
    "\n",
    "def getVerbsFromConjunctions(verbs):\n",
    "    moreVerbs = []\n",
    "    for verb in verbs:\n",
    "        rightDeps = {tok.lower_ for tok in verb.rights}\n",
    "        if \"and\" in rightDeps:\n",
    "            moreVerbs.extend([tok for tok in verb.rights if tok.pos_ == \"VERB\"])\n",
    "            if len(moreVerbs) > 0:\n",
    "                moreVerbs.extend(getVerbsFromConjunctions(moreVerbs))\n",
    "    return moreVerbs\n",
    "\n",
    "def findSubs(tok):\n",
    "    head = tok.head\n",
    "    while head.pos_ != \"VERB\" and head.pos_ != \"NOUN\" and head.head != head:\n",
    "        head = head.head\n",
    "    if head.pos_ == \"VERB\":\n",
    "        subs = [tok for tok in head.lefts if tok.dep_ == \"SUB\"]\n",
    "        if len(subs) > 0:\n",
    "            verbNegated = isNegated(head)\n",
    "            subs.extend(getSubsFromConjunctions(subs))\n",
    "            return subs, verbNegated\n",
    "        elif head.head != head:\n",
    "            return findSubs(head)\n",
    "    elif head.pos_ == \"NOUN\":\n",
    "        return [head], isNegated(tok)\n",
    "    return [], False\n",
    "\n",
    "def isNegated(tok):\n",
    "    negations = {\"no\", \"not\", \"n't\", \"never\", \"none\"}\n",
    "    for dep in list(tok.lefts) + list(tok.rights):\n",
    "        if dep.lower_ in negations:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def find_negation(tok):\n",
    "    negations = {\"no\", \"not\", \"n't\", \"never\", \"none\"}\n",
    "    for dep in list(tok.lefts):\n",
    "        if dep.lower_ in negations:\n",
    "            verb = dep.lower_ + \" \" + tok.lemma_\n",
    "            verb_id = [dep.i, tok.i]\n",
    "            return verb, verb_id\n",
    "    verb = tok.lemma_\n",
    "    verb_id = [tok.i]\n",
    "    return verb, verb_id\n",
    "\n",
    "def getObjsFromPrepositions(deps):\n",
    "    objs = []\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"ADP\" and (dep.dep_ == \"prep\" or dep.dep_ == \"agent\"):\n",
    "            for tok in dep.rights:\n",
    "                if (tok.pos_ == \"NOUN\" and tok.dep_ in OBJECTS) or (tok.pos_ == \"PRON\" and tok.lower_ == \"me\"):\n",
    "                    objs.append(tok)\n",
    "                elif tok.dep_ == \"pcomp\":\n",
    "                    for t in tok.rights:\n",
    "                        if (t.pos_ == \"NOUN\" and t.dep_ in OBJECTS) or (t.pos_ == \"PRON\" and t.lower_ == \"me\"):\n",
    "                            objs.append(t)\n",
    "                else:\n",
    "                    objs.extend(getObjsFromPrepositions(tok.rights))\n",
    "    return objs\n",
    "\n",
    "def getAdjectives(toks):\n",
    "    toks_with_adjectives = []\n",
    "    for tok in toks:\n",
    "        adjs = [left for left in tok.lefts if left.dep_ in ADJECTIVES]\n",
    "        adjs.append(tok)\n",
    "        adjs.extend([right for right in tok.rights if tok.dep_ in ADJECTIVES])\n",
    "        tok_with_adj = \" \".join([adj.lower_ for adj in adjs])\n",
    "        toks_with_adjectives.extend(adjs)\n",
    "    return toks_with_adjectives\n",
    "\n",
    "def getObjsFromAttrs(deps):\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"NOUN\" and dep.dep_ == \"attr\":\n",
    "            verbs = [tok for tok in dep.rights if tok.pos_ == \"VERB\"]\n",
    "            if len(verbs) > 0:\n",
    "                for v in verbs:\n",
    "                    rights = list(v.rights)\n",
    "                    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "                    objs.extend(getObjsFromPrepositions(rights))\n",
    "                    if len(objs) > 0:\n",
    "                        return v, objs\n",
    "    return None, None\n",
    "\n",
    "def getObjFromXComp(deps):\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"VERB\" and dep.dep_ == \"xcomp\":\n",
    "            v = dep\n",
    "            rights = list(v.rights)\n",
    "            objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "            objs.extend(getObjsFromPrepositions(rights))\n",
    "            if len(objs) > 0:\n",
    "                return v, objs\n",
    "    return None, None\n",
    "def getAllSubs(v):\n",
    "    verbNegated = isNegated(v)\n",
    "    subs = [tok for tok in v.lefts if tok.dep_ in SUBJECTS and tok.pos_ != \"DET\"]\n",
    "    if len(subs) > 0:\n",
    "        subs.extend(getSubsFromConjunctions(subs))\n",
    "    else:\n",
    "        foundSubs, verbNegated = findSubs(v)\n",
    "        subs.extend(foundSubs)\n",
    "    return subs, verbNegated\n",
    "\n",
    "def getAllObjs(v):\n",
    "    rights = list(v.rights)\n",
    "    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "    objs.extend(getObjsFromPrepositions(rights))\n",
    "    potentialNewVerb, potentialNewObjs = getObjFromXComp(rights)\n",
    "    if (potentialNewVerb is not None and potentialNewObjs is not None and len(potentialNewObjs) > 0):\n",
    "        objs.extend(potentialNewObjs)\n",
    "        v = potentialNewVerb\n",
    "    if len(objs) > 0:\n",
    "        objs.extend(getObjsFromConjunctions(objs))\n",
    "    else:\n",
    "        objs.extend(getObjsFromVerbConj(v))\n",
    "    return v, objs\n",
    "\n",
    "def getAllObjsWithAdjectives(v):\n",
    "    rights = list(v.rights)\n",
    "    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "    if len(objs) == 0:\n",
    "        objs = [tok for tok in rights if tok.dep_ in ADJECTIVES]\n",
    "    objs.extend(getObjsFromPrepositions(rights))\n",
    "    potentialNewVerb, potentialNewObjs = getObjFromXComp(rights)\n",
    "    if (potentialNewVerb is not None and potentialNewObjs is not None and len(potentialNewObjs) > 0):\n",
    "        objs.extend(potentialNewObjs)\n",
    "        v = potentialNewVerb\n",
    "    if len(objs) > 0:\n",
    "        objs.extend(getObjsFromConjunctions(objs))\n",
    "    else:\n",
    "        objs.extend(getObjsFromVerbConj(v))\n",
    "    return v, objs\n",
    "\n",
    "def getObjsFromVerbConj(v):\n",
    "    objs = []\n",
    "    rights = list(v.rights)\n",
    "    for right in rights:\n",
    "        if right.dep_ == \"conj\":\n",
    "            subs, verbNegated = getAllSubs(right)\n",
    "            objs.extend(subs)\n",
    "        else:\n",
    "            objs.extend(getObjsFromVerbConj(right))\n",
    "    return objs\n",
    "\n",
    "def check_tag(compound):\n",
    "    flag = False\n",
    "    res = \"\"\n",
    "    for token in compound:\n",
    "        if token.ent_type_ == \"PERSON\":\n",
    "            flag = True\n",
    "            res = \"<NAME>\"\n",
    "            break\n",
    "        elif token.ent_type_ == \"ORG\":\n",
    "            flag = True\n",
    "            res = \"<ORG>\"\n",
    "            break\n",
    "    return flag, res\n",
    "\n",
    "def generate_compound(token):\n",
    "    token_compunds = []\n",
    "    for tok in token.lefts:\n",
    "        if tok.dep_ in COMPOUNDS:\n",
    "            token_compunds.extend(generate_compound(tok))\n",
    "    token_compunds.append(token)\n",
    "    for tok in token.rights:\n",
    "        if tok.dep_ in COMPOUNDS:\n",
    "            token_compunds.extend(generate_compound(tok))\n",
    "    return token_compunds\n",
    "\n",
    "def generate_verb_advmod(v):\n",
    "    v_compunds = []\n",
    "    for tok in v.lefts:\n",
    "        if tok.dep_ in ADVERBS:\n",
    "            v_compunds.extend(generate_verb_advmod(tok))\n",
    "    v_compunds.append(v)\n",
    "    for tok in v.rights:\n",
    "        if tok.dep_ in ADVERBS:\n",
    "            v_compunds.extend(generate_verb_advmod(tok))\n",
    "    return v_compunds\n",
    "\n",
    "def generate_left_right_adjectives(obj):\n",
    "    obj_desc_tokens = []\n",
    "    for tok in obj.lefts:\n",
    "        if tok.dep_ in ADJECTIVES:\n",
    "            obj_desc_tokens.extend(generate_left_right_adjectives(tok))\n",
    "    obj_desc_tokens.append(obj)\n",
    "    for tok in obj.rights:\n",
    "        if tok.dep_ in ADJECTIVES:\n",
    "            obj_desc_tokens.extend(generate_left_right_adjectives(tok))\n",
    "    return obj_desc_tokens\n",
    "\n",
    "def findSVOs(tokens, len_doc):\n",
    "    svos = []\n",
    "    svo_token_ids = []\n",
    "    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\" and tok.dep_ != \"aux\"]\n",
    "    for v in verbs:\n",
    "        subs, verbNegated = getAllSubs(v)\n",
    "        verb, verb_id = find_negation(v)\n",
    "        if len(subs) > 0:\n",
    "            v, objs = getAllObjs(v)\n",
    "            for sub in subs:\n",
    "                for obj in objs:\n",
    "                    sub_compound = generate_compound(sub)\n",
    "                    obj_compound = generate_compound(obj)\n",
    "                    sub_flag, sub_tag = check_tag(sub_compound)\n",
    "                    obj_flag, obj_tag = check_tag(obj_compound)\n",
    "                    if obj_flag and sub_flag:\n",
    "                        event = (sub_tag, verb, obj_tag)\n",
    "                    elif obj_flag:\n",
    "                        event = (\" \".join(tok.lemma_ for tok in sub_compound), verb, obj_tag)\n",
    "                    elif sub_flag:\n",
    "                        event = (sub_tag, verb, \" \".join(tok.lemma_ for tok in obj_compound))\n",
    "                    else:\n",
    "                        event = (\" \".join(tok.lemma_ for tok in sub_compound), verb, \" \".join(tok.lemma_ for tok in obj_compound))\n",
    "                    svos.append(event)\n",
    "    return svos, svo_token_ids\n",
    "\n",
    "single_words = [\"a\", \"A\", \"<\", \">\", \"i\", \"I\"]\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    regex = re.compile(\"[^a-zA-Z<>.\\s]\")\n",
    "    text_returned = re.sub(regex, \" \", text)\n",
    "    tokens = text_returned.split()\n",
    "    words = []\n",
    "    for word in tokens:\n",
    "        if len(word) > 1 or word in single_words:\n",
    "            words.append(word)\n",
    "    out = \" \".join(words)\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Updated events extraction function for single text processing\n",
    "def extract_events_from_text(content):\n",
    "    content = preprocess(content)\n",
    "    pattern = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s'\n",
    "    content_sents = re.split(pattern, content)\n",
    "    file_svo_text = []\n",
    "    lines = []\n",
    "    for i, line in enumerate(content_sents):\n",
    "        line = line.strip()\n",
    "        lines.append(remove_special_characters(line))\n",
    "    for i, doc in enumerate(nlp.pipe(lines)):\n",
    "        SVO, SVO_Token_IDs = findSVOs(doc, 0)\n",
    "        if len(SVO) > 0:\n",
    "            for eve in SVO:\n",
    "                file_svo_text.append(\" \".join(eve))\n",
    "    # If no event found, fallback: use original string (optional, else return empty string or list)\n",
    "    if not file_svo_text:\n",
    "        return \"\"\n",
    "    return \" ||| \".join(file_svo_text)\n",
    "\n",
    "# MAIN SCRIPT\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv = \"filtered_all_removed_conclusion_source.csv\"\n",
    "    output_csv = \"filtered_all_events_removed_conclusion_source.csv\"\n",
    "    \n",
    "    df = pd.read_csv(input_csv)\n",
    "    tqdm.pandas(desc=\"Extracting source events\")\n",
    "    df['source_event'] = df['source_text'].progress_apply(extract_events_from_text)\n",
    "    tqdm.pandas(desc=\"Extracting target events\")\n",
    "    df['target_event'] = df['target_text'].progress_apply(extract_events_from_text)\n",
    "    # Keep rest of columns as they are, but place new columns at the start (optional)\n",
    "    output_cols = ['source_event', 'target_event', 'relation', 'source_type', 'target_type', 'file_name']\n",
    "    # Insert at correct positions, original order with two new columns up front (or edit as you want)\n",
    "    df = df[['source_event', 'target_event', 'relation', 'source_type', 'target_type', 'file_name']]\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Saved extracted events to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8abd85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
