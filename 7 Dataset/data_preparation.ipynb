{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "816bb6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9338261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_similarity(a, b):\n",
    "    return SequenceMatcher(None, a.strip().lower(), b.strip().lower()).ratio()\n",
    "\n",
    "def parse_annotated_file(annotated_path):\n",
    "    \"\"\"Parse annotated text file into segments\"\"\"\n",
    "    segments = []\n",
    "    with open(annotated_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.split('||')\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            text = parts[0].strip()\n",
    "            label = parts[1].strip()\n",
    "            \n",
    "            if label == 'NA':\n",
    "                segments.append({'text': text, 'type': 'non-argumentative', 'id': None})\n",
    "            elif label.startswith('P_'):\n",
    "                segments.append({'text': text, 'type': 'premise', 'id': label[2:]})\n",
    "            elif label.startswith('C_'):\n",
    "                segments.append({'text': text, 'type': 'conclusion', 'id': label[2:]})\n",
    "    return segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2afa549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml_relations(xml_path):\n",
    "    \"\"\"Extract relations from XML file\"\"\"\n",
    "    tree = ET.parse(xml_path)\n",
    "    relations = []\n",
    "    \n",
    "    for elem in tree.iter():\n",
    "        if elem.tag not in ['prem', 'conc']:\n",
    "            continue\n",
    "            \n",
    "        source_id = elem.attrib['ID']\n",
    "        \n",
    "        # Process supports\n",
    "        for target in elem.attrib.get('SUP', '').split('|'):\n",
    "            if target:\n",
    "                relations.append(('support', source_id, target))\n",
    "        \n",
    "        # Process attacks\n",
    "        for target in elem.attrib.get('ATT', '').split('|'):\n",
    "            if target:\n",
    "                relations.append(('attack', source_id, target))\n",
    "    \n",
    "    return relations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9be2194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_id_mapping(segments):\n",
    "    \"\"\"Create mapping from segment text to ID with fuzzy matching\"\"\"\n",
    "    id_map = {}\n",
    "    for seg in segments:\n",
    "        if seg['id'] is None:\n",
    "            continue\n",
    "        existing = id_map.get(seg['id'])\n",
    "        if existing is None or text_similarity(seg['text'], existing['text']) < 0.8:\n",
    "            id_map[seg['id']] = seg\n",
    "    return id_map\n",
    "\n",
    "def process_document(base_name, data_folder):\n",
    "    \"\"\"Process a single judgment document\"\"\"\n",
    "    # Load annotated text\n",
    "    annotated_path = os.path.join(data_folder, 'annotated', f\"{base_name}___annotated_judgment.txt\")\n",
    "    if not os.path.exists(annotated_path):\n",
    "        return []\n",
    "    \n",
    "    segments = parse_annotated_file(annotated_path)\n",
    "    id_map = create_id_mapping(segments)\n",
    "    \n",
    "    # Load XML relations\n",
    "    xml_path = os.path.join(data_folder, 'xml_files', f\"{base_name}.xml\")\n",
    "    if not os.path.exists(xml_path):\n",
    "        return []\n",
    "    \n",
    "    relations = parse_xml_relations(xml_path)\n",
    "    \n",
    "    # Generate pairs\n",
    "    pairs = []\n",
    "    for i, src in enumerate(segments):\n",
    "        for j, tgt in enumerate(segments):\n",
    "            if i == j:\n",
    "                continue\n",
    "                \n",
    "            # Determine relation\n",
    "            relation = 'no-relation'\n",
    "            if src['type'] != 'non-argumentative' and tgt['type'] != 'non-argumentative':\n",
    "                src_id = src['id']\n",
    "                tgt_id = tgt['id']\n",
    "                \n",
    "                # Find matching relations\n",
    "                for rel_type, s_id, t_id in relations:\n",
    "                    if (text_similarity(src_id, s_id) > 0.8 and \n",
    "                        text_similarity(tgt_id, t_id) > 0.8):\n",
    "                        relation = rel_type\n",
    "                        break\n",
    "            \n",
    "            pairs.append({\n",
    "                'source_text': src['text'],\n",
    "                'target_text': tgt['text'],\n",
    "                'relation': relation,\n",
    "                'source_type': src['type'],\n",
    "                'target_type': tgt['type'],\n",
    "                'file_name': base_name\n",
    "            })\n",
    "    \n",
    "    return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51aeea73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [01:13<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created with 1601063 pairs. Saved to final_relations_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "data_folder = './'\n",
    "output_file = 'final_relations_dataset.csv'\n",
    "\n",
    "# Get list of annotated files\n",
    "annotated_files = [f for f in os.listdir(os.path.join(data_folder, 'annotated')) \n",
    "                    if f.endswith('___annotated_judgment.txt')]\n",
    "\n",
    "# Process all documents\n",
    "all_pairs = []\n",
    "for annotated_file in tqdm(annotated_files):\n",
    "    base_name = annotated_file.replace('___annotated_judgment.txt', '')\n",
    "    pairs = process_document(base_name, data_folder)\n",
    "    all_pairs.extend(pairs)\n",
    "\n",
    "# Create DataFrame and save\n",
    "df = pd.DataFrame(all_pairs)\n",
    "df = df.drop_duplicates(subset=['source_text', 'target_text', 'file_name'])\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Dataset created with {len(df)} pairs. Saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd99426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc45c5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before enhanced cleaning:\n",
      "Total rows: 1601063\n",
      "Sample rows with numbers:\n",
      "    source_text                     target_text     relation  \\\n",
      "497         12.  1Language of the case: French.  no-relation   \n",
      "498         12.                  JUDGMENT OF 8.  no-relation   \n",
      "\n",
      "           source_type        target_type  \\\n",
      "497  non-argumentative  non-argumentative   \n",
      "498  non-argumentative  non-argumentative   \n",
      "\n",
      "                                         file_name  \n",
      "497  R2011_France Télécom SA v European Commission  \n",
      "498  R2011_France Télécom SA v European Commission  \n",
      "                        source_text target_text     relation  \\\n",
      "1    1Language of the case: French.         12.  no-relation   \n",
      "243  1Language of the case: French.          1.  no-relation   \n",
      "244  1Language of the case: French.          2.  no-relation   \n",
      "\n",
      "           source_type        target_type  \\\n",
      "1    non-argumentative  non-argumentative   \n",
      "243  non-argumentative  non-argumentative   \n",
      "244  non-argumentative  non-argumentative   \n",
      "\n",
      "                                         file_name  \n",
      "1    R2011_France Télécom SA v European Commission  \n",
      "243  R2011_France Télécom SA v European Commission  \n",
      "244  R2011_France Télécom SA v European Commission  \n",
      "\n",
      "After enhanced cleaning:\n",
      "Removed 497468 rows\n",
      "Remaining rows: 1103595\n",
      "Sample cleaned rows:\n",
      "                     source_text  \\\n",
      "2  Language of the case: French.   \n",
      "3  Language of the case: French.   \n",
      "4  Language of the case: French.   \n",
      "\n",
      "                                         target_text     relation  \\\n",
      "2  State aid paid by France to FT (OJ 2005 L 269,...  no-relation   \n",
      "3                           in the General Tax Code.  no-relation   \n",
      "4        a self-employed occupation as at 1 January.  no-relation   \n",
      "\n",
      "         source_type        target_type  \\\n",
      "2  non-argumentative  non-argumentative   \n",
      "3  non-argumentative  non-argumentative   \n",
      "4  non-argumentative  non-argumentative   \n",
      "\n",
      "                                       file_name  \n",
      "2  R2011_France Télécom SA v European Commission  \n",
      "3  R2011_France Télécom SA v European Commission  \n",
      "4  R2011_France Télécom SA v European Commission  \n"
     ]
    }
   ],
   "source": [
    "# %% [Cell 1] Add Enhanced Cleaning Functions\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def remove_leading_numbers(text):\n",
    "    \"\"\"Remove 1-3 digit numbers at start of text with trailing period/space\"\"\"\n",
    "    return re.sub(r'^\\d{1,3}\\.*\\s*', '', text).strip()\n",
    "\n",
    "def filter_short_phrases(df):\n",
    "    \"\"\"Remove rows with fewer than 3 words in source_text or target_text\"\"\"\n",
    "    return df[\n",
    "        (df['source_text'].apply(lambda x: len(str(x).split()) >= 4)) &\n",
    "        (df['target_text'].apply(lambda x: len(str(x).split()) >= 4))\n",
    "    ]\n",
    "\n",
    "# %% [Cell 2] Enhanced Processing Pipeline\n",
    "def enhanced_processing(input_csv, output_csv):\n",
    "    # Load cleaned dataset\n",
    "    df = pd.read_csv(input_csv)\n",
    "    \n",
    "    print(\"Before enhanced cleaning:\")\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    print(\"Sample rows with numbers:\")\n",
    "    print(df[df['source_text'].str.match(r'^\\d{1,3}\\.')].head(2))\n",
    "    print(df[df['target_text'].str.match(r'^\\d{1,3}\\.')].head(3))\n",
    "\n",
    "        \n",
    "    # Remove leading numbers\n",
    "    df['source_text'] = df['source_text'].apply(remove_leading_numbers)\n",
    "    df['target_text'] = df['target_text'].apply(remove_leading_numbers)\n",
    "        \n",
    "    # Filter short phrases\n",
    "    initial_count = len(df)\n",
    "    df = filter_short_phrases(df)\n",
    "    \n",
    "    print(\"\\nAfter enhanced cleaning:\")\n",
    "    print(f\"Removed {initial_count - len(df)} rows\")\n",
    "    print(f\"Remaining rows: {len(df)}\")\n",
    "    print(\"Sample cleaned rows:\")\n",
    "    print(df.head(3))\n",
    "    \n",
    "    # Save final dataset\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    return df\n",
    "\n",
    "final_df = enhanced_processing('final_relations_dataset.csv', 'final_relations_dataset_cleaned.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcfdda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall relation counts (1103595 total pairs):\n",
      "               count\n",
      "no-relation  1101631\n",
      "support         1846\n",
      "attack           118\n",
      "\n",
      "Per-file relation counts:\n",
      "relation                                            attack  no-relation  \\\n",
      "file_name                                                                 \n",
      "A2008_Commission of the European Communities v ...       2        23823   \n",
      "A2009_3F v Commission of the European Communities        4        25699   \n",
      "A2009_Commission of the European Communities v ...       0        16482   \n",
      "A2010_NDSHT Nya Destination Stockholm Hotell & ...       0         9097   \n",
      "A2011_European Commission (C-106_09 P) and King...       2        70994   \n",
      "A2012_BNP Paribas and Banca Nazionale del Lavor...       3        49478   \n",
      "A2013_European Commission v Ireland and Others           0         9266   \n",
      "A2013_Frucona Košice a.s. v European Commission          1        30059   \n",
      "A2016_European Commission v Aer Lingus Ltd and ...       2        40139   \n",
      "A2016_European_Commission_v_World_Duty_Free              1        26013   \n",
      "A2017_Ellinikos Chrysos AE Metalleion kai Viomi...       1         7634   \n",
      "A2017_European Commission v Italian Republic_DT          1        12150   \n",
      "A2017_European Commission v TV2_Danmark A_S              1        14732   \n",
      "A2018_Commission v Spain                                 5        50770   \n",
      "A2018_Dirk Andres v European Commission                  4        22598   \n",
      "A2018_Scuola Elementare Maria Montessori Srl v ...       3        25036   \n",
      "R1997_Tiercé Ladbroke SA v Commission of the Eu...       4         7458   \n",
      "R2000_French Republic v Ladbroke Racing Ltd and...       8        10476   \n",
      "R2004_Daewoo Electronics Manufacturing España S...       0        20587   \n",
      "R2004_Italian Republic v Commission of the Euro...       0        27697   \n",
      "R2004_Ramondín SA and Ramondín Cápsulas SA (C-1...       0         9489   \n",
      "R2006_European Commission v Italian Republic             1        14723   \n",
      "R2010_AceaElectrabel Produzione SpA v European ...       0       131347   \n",
      "R2011_European Commission v Kronoply GmbH & Co           0         8527   \n",
      "R2011_France Télécom SA v European Commission            0        41357   \n",
      "R2012_European Commission v Électricité de Fran...       5        55406   \n",
      "R2013_3F, formerly Specialarbejderforbundet i D...       1        14977   \n",
      "R2013_Telefónica SA v European Commission                0         8902   \n",
      "R2015_European Commission v MOL Magyar Olaj- és...       0        15224   \n",
      "R2016_DTS Distribuidora de Televisión Digital            6        50307   \n",
      "R2016_European Commission v Hansestadt Lübeck            0        19674   \n",
      "R2016_Hellenic Republic v European Commission            0        13290   \n",
      "R2016_Netherlands Maritime Technology Associati...       8        33237   \n",
      "R2016_Orange v European Commission                       9        19376   \n",
      "R2017_European Commission v Frucona Košice a             0        31096   \n",
      "R2017_Viasat Broadcasting UK Ltd v European Com...       3        14231   \n",
      "R2021_FVE Holýšov I and Others v Commission              7        23219   \n",
      "R2021_Prosegur Compañía de Seguridad SA, establ...      24        55320   \n",
      "R2021_World Duty Free v. Commission                     12        41741   \n",
      "\n",
      "relation                                            support  \n",
      "file_name                                                    \n",
      "A2008_Commission of the European Communities v ...       47  \n",
      "A2009_3F v Commission of the European Communities        59  \n",
      "A2009_Commission of the European Communities v ...       32  \n",
      "A2010_NDSHT Nya Destination Stockholm Hotell & ...       23  \n",
      "A2011_European Commission (C-106_09 P) and King...       30  \n",
      "A2012_BNP Paribas and Banca Nazionale del Lavor...       26  \n",
      "A2013_European Commission v Ireland and Others           46  \n",
      "A2013_Frucona Košice a.s. v European Commission          43  \n",
      "A2016_European Commission v Aer Lingus Ltd and ...       61  \n",
      "A2016_European_Commission_v_World_Duty_Free              70  \n",
      "A2017_Ellinikos Chrysos AE Metalleion kai Viomi...       21  \n",
      "A2017_European Commission v Italian Republic_DT          59  \n",
      "A2017_European Commission v TV2_Danmark A_S              30  \n",
      "A2018_Commission v Spain                                 75  \n",
      "A2018_Dirk Andres v European Commission                  49  \n",
      "A2018_Scuola Elementare Maria Montessori Srl v ...       85  \n",
      "R1997_Tiercé Ladbroke SA v Commission of the Eu...       22  \n",
      "R2000_French Republic v Ladbroke Racing Ltd and...       23  \n",
      "R2004_Daewoo Electronics Manufacturing España S...       16  \n",
      "R2004_Italian Republic v Commission of the Euro...       25  \n",
      "R2004_Ramondín SA and Ramondín Cápsulas SA (C-1...       18  \n",
      "R2006_European Commission v Italian Republic             38  \n",
      "R2010_AceaElectrabel Produzione SpA v European ...       64  \n",
      "R2011_European Commission v Kronoply GmbH & Co           29  \n",
      "R2011_France Télécom SA v European Commission            56  \n",
      "R2012_European Commission v Électricité de Fran...       52  \n",
      "R2013_3F, formerly Specialarbejderforbundet i D...       29  \n",
      "R2013_Telefónica SA v European Commission                29  \n",
      "R2015_European Commission v MOL Magyar Olaj- és...       29  \n",
      "R2016_DTS Distribuidora de Televisión Digital            87  \n",
      "R2016_European Commission v Hansestadt Lübeck            66  \n",
      "R2016_Hellenic Republic v European Commission            51  \n",
      "R2016_Netherlands Maritime Technology Associati...       61  \n",
      "R2016_Orange v European Commission                       75  \n",
      "R2017_European Commission v Frucona Košice a             56  \n",
      "R2017_Viasat Broadcasting UK Ltd v European Com...       46  \n",
      "R2021_FVE Holýšov I and Others v Commission              30  \n",
      "R2021_Prosegur Compañía de Seguridad SA, establ...      118  \n",
      "R2021_World Duty Free v. Commission                      70  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Verify dataset file exists\n",
    "dataset_file = 'final_relations_dataset_cleaned.csv'\n",
    "if not os.path.exists(dataset_file):\n",
    "    print(\"Error: Dataset file not found. Existing files:\")\n",
    "    print('\\n'.join(f for f in os.listdir('.') if f.endswith('.csv')))\n",
    "else:\n",
    "    df = pd.read_csv(dataset_file)\n",
    "    \n",
    "    # Overall statistics\n",
    "    overall = df['relation'].value_counts().to_dict()\n",
    "    \n",
    "    # Per-file statistics\n",
    "    per_file = df.groupby('file_name')['relation'].value_counts().unstack(fill_value=0)\n",
    "    \n",
    "    print(f\"\\nOverall relation counts ({len(df)} total pairs):\")\n",
    "    print(pd.DataFrame.from_dict(overall, orient='index', columns=['count']))\n",
    "    \n",
    "    print(\"\\nPer-file relation counts:\")\n",
    "    print(per_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "074d876a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset composition:\n",
      "relation\n",
      "no-relation    2925\n",
      "support        1846\n",
      "attack          118\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total samples: 4889\n",
      "Saved to: balanced_relations_dataset.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1999920/2723469136.py:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=min(75, len(x)),\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned dataset\n",
    "input_file = \"final_relations_dataset_cleaned.csv\"\n",
    "output_file = \"balanced_relations_dataset.csv\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Separate relations\n",
    "    support = df[df['relation'] == 'support']\n",
    "    attack = df[df['relation'] == 'attack']\n",
    "    no_relation = df[df['relation'] == 'no-relation']\n",
    "\n",
    "    # Sample 75 no-relation per file\n",
    "    sampled_no_relation = no_relation.groupby('file_name', group_keys=False)\\\n",
    "                                    .apply(lambda x: x.sample(n=min(75, len(x)), \n",
    "                                                             random_state=42))\n",
    "\n",
    "    # Combine all relations\n",
    "    balanced_df = pd.concat([support, attack, sampled_no_relation], ignore_index=True)\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Save to new file\n",
    "    balanced_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Show statistics\n",
    "    print(\"Final dataset composition:\")\n",
    "    print(balanced_df['relation'].value_counts())\n",
    "    print(f\"\\nTotal samples: {len(balanced_df)}\")\n",
    "    print(f\"Saved to: {output_file}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File '{input_file}' not found. Please verify:\")\n",
    "    print(\"1. The file exists in the current directory\")\n",
    "    print(\"2. The filename is spelled correctly\")\n",
    "    print(\"3. The file path is correct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4042897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def analyze_relations_dataset(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Create type pair column\n",
    "        df['type_pair'] = df['source_type'] + '-' + df['target_type']\n",
    "        \n",
    "        # 1. Overall relation counts\n",
    "        relation_counts = df['relation'].value_counts().to_dict()\n",
    "        \n",
    "        # 2. Type pair distribution\n",
    "        type_pair_counts = df['type_pair'].value_counts().to_dict()\n",
    "        \n",
    "        # 3. Relation breakdown per type pair\n",
    "        pair_relations = df.groupby(['type_pair', 'relation']).size().unstack(fill_value=0)\n",
    "        \n",
    "        # 4. Detailed metrics\n",
    "        metrics = {\n",
    "            'premise-premise': {\n",
    "                'support': pair_relations.loc['premise-premise', 'support'] if 'premise-premise' in pair_relations.index else 0,\n",
    "                'attack': pair_relations.loc['premise-premise', 'attack'] if 'premise-premise' in pair_relations.index else 0,\n",
    "                'total': type_pair_counts.get('premise-premise', 0)\n",
    "            },\n",
    "            'premise-conclusion': {\n",
    "                'support': pair_relations.loc['premise-conclusion', 'support'] if 'premise-conclusion' in pair_relations.index else 0,\n",
    "                'attack': pair_relations.loc['premise-conclusion', 'attack'] if 'premise-conclusion' in pair_relations.index else 0,\n",
    "                'total': type_pair_counts.get('premise-conclusion', 0)\n",
    "            },\n",
    "            'conclusion-premise': {\n",
    "                'support': pair_relations.loc['conclusion-premise', 'support'] if 'conclusion-premise' in pair_relations.index else 0,\n",
    "                'attack': pair_relations.loc['conclusion-premise', 'attack'] if 'conclusion-premise' in pair_relations.index else 0,\n",
    "                'total': type_pair_counts.get('conclusion-premise', 0)\n",
    "            },\n",
    "            'conclusion-conclusion': {\n",
    "                'support': pair_relations.loc['conclusion-conclusion', 'support'] if 'conclusion-conclusion' in pair_relations.index else 0,\n",
    "                'attack': pair_relations.loc['conclusion-conclusion', 'attack'] if 'conclusion-conclusion' in pair_relations.index else 0,\n",
    "                'total': type_pair_counts.get('conclusion-conclusion', 0)\n",
    "            },\n",
    "            'non-argumentative-premise': {\n",
    "                'total': type_pair_counts.get('non-argumentative-premise', 0)\n",
    "            },\n",
    "            'non-argumentative-conclusion': {\n",
    "                'total': type_pair_counts.get('non-argumentative-conclusion', 0)\n",
    "            },\n",
    "            'premise-non-argumentative': {\n",
    "                'total': type_pair_counts.get('premise-non-argumentative', 0)\n",
    "            },\n",
    "            'conclusion-non-argumentative': {\n",
    "                'total': type_pair_counts.get('conclusion-non-argumentative', 0)\n",
    "            },\n",
    "            'non-argumentative-non-argumentative': {\n",
    "                'total': type_pair_counts.get('non-argumentative-non-argumentative', 0)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Calculate percentages\n",
    "        total_pairs = len(df)\n",
    "        for pair in metrics:\n",
    "            if 'total' in metrics[pair]:\n",
    "                metrics[pair]['percentage'] = (metrics[pair]['total'] / total_pairs) * 100\n",
    "        \n",
    "        return {\n",
    "            'total_pairs': total_pairs,\n",
    "            'relation_distribution': relation_counts,\n",
    "            'type_pair_metrics': metrics,\n",
    "            'detailed_breakdown': pair_relations.to_dict()\n",
    "        }\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "        print(\"Please verify:\")\n",
    "        print(\"1. The file exists in the current directory\")\n",
    "        print(\"2. You're using the correct filename (case-sensitive)\")\n",
    "        print(\"3. The file path is correct\")\n",
    "        return None\n",
    "\n",
    "# Run analysis\n",
    "results = analyze_relations_dataset('balanced_relations_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f71e7ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Pairs: 4889\n",
      "\n",
      "Relation Distribution:\n",
      "             Count\n",
      "no-relation   2925\n",
      "support       1846\n",
      "attack         118\n",
      "\n",
      "Type Pair Metrics:\n",
      "                                     support  attack   total  percentage\n",
      "premise-premise                       1516.0   105.0  2019.0   41.296789\n",
      "premise-conclusion                       0.0    13.0    39.0    0.797709\n",
      "conclusion-premise                     330.0     0.0   352.0    7.199836\n",
      "conclusion-conclusion                    0.0     0.0     1.0    0.020454\n",
      "non-argumentative-premise                NaN     NaN   594.0   12.149724\n",
      "non-argumentative-conclusion             NaN     NaN    43.0    0.879525\n",
      "premise-non-argumentative                NaN     NaN   601.0   12.292902\n",
      "conclusion-non-argumentative             NaN     NaN    36.0    0.736347\n",
      "non-argumentative-non-argumentative      NaN     NaN  1204.0   24.626713\n"
     ]
    }
   ],
   "source": [
    "if results:\n",
    "    print(\"\\nTotal Pairs:\", results['total_pairs'])\n",
    "    print(\"\\nRelation Distribution:\")\n",
    "    print(pd.DataFrame.from_dict(results['relation_distribution'], orient='index', columns=['Count']))\n",
    "    \n",
    "    print(\"\\nType Pair Metrics:\")\n",
    "    print(pd.DataFrame(results['type_pair_metrics']).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccc9633",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
