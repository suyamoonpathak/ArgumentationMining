{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6284e95b",
   "metadata": {},
   "source": [
    "# ucreat events extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91cae103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting source events: 100%|██████████| 4664/4664 [00:54<00:00, 85.22it/s]\n",
      "Extracting target events: 100%|██████████| 4664/4664 [00:54<00:00, 86.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved extracted events to filtered_all_events_removed_conclusion_source.csv\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Load spaCy model (assumes GPU is available)\n",
    "spacy.require_gpu()\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "alphabet_string = string.ascii_lowercase\n",
    "alphabet_list = list(alphabet_string)\n",
    "exclusion_list = alphabet_list + [\n",
    "    \"no\", \"nos\", \"sub-s\", \"subs\", \"ss\", \"cl\", \"dr\", \"mr\", \"mrs\", \"dr\", \"vs\", \"ch\", \"addl\",\n",
    "]\n",
    "exclusion_list = [word + \".\" for word in exclusion_list]\n",
    "\n",
    "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
    "OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\", \"pobj\"]\n",
    "ADJECTIVES = [\"acomp\", \"advcl\", \"advmod\", \"amod\", \"appos\", \"nn\", \"nmod\", \"ccomp\", \"complm\", \"hmod\", \"infmod\", \"xcomp\", \"rcmod\", \"poss\", \" possessive\"]\n",
    "ADVERBS = [\"advmod\"]\n",
    "COMPOUNDS = [\"compound\"]\n",
    "PREPOSITIONS = [\"prep\"]\n",
    "\n",
    "def preprocess(content):\n",
    "    raw_text = re.sub(r\"\\xa0\", \" \", str(content))\n",
    "    raw_text = raw_text.split(\"\\n\")\n",
    "    text = raw_text.copy()\n",
    "    text = [re.sub(r'[^a-zA-Z0-9.,<>)\\-(/?\\t ]', '', sentence) for sentence in text]\n",
    "    text = [re.sub(\"\\t+\", \" \", sentence) for sentence in text]\n",
    "    text = [re.sub(\"\\s+\", \" \", sentence) for sentence in text]\n",
    "    text = [re.sub(\" +\", \" \", sentence) for sentence in text]\n",
    "    text = [re.sub(\"\\.\\.+\", \"\", sentence) for sentence in text]\n",
    "    text = [re.sub(\"\\A ?\", \"\", sentence) for sentence in text]\n",
    "    text = [sentence for sentence in text if(len(sentence) != 1 and not re.fullmatch(\"(\\d|\\d\\d|\\d\\d\\d)\", sentence))]\n",
    "    text = [sentence for sentence in text if len(sentence) != 0]\n",
    "    text = [re.sub('\\A\\(?(\\d|\\d\\d\\d|\\d\\d|[a-zA-Z])(\\.|\\))\\s?(?=[A-Z])', '\\n', sentence) for sentence in text]\n",
    "    text = [re.sub(\"\\A\\(([ivx]+)\\)\\s?(?=[a-zA-Z0-9])\", '\\n', sentence) for sentence in text]\n",
    "    text = [re.sub(r\"[()[\\]\\\"$']\", \" \", sentence) for sentence in text]\n",
    "    text = [re.sub(r\" no.\", \" number \", sentence, flags=re.I) for sentence in text]\n",
    "    text = [re.sub(r\" nos.\", \" numbers \", sentence, flags=re.I) for sentence in text]\n",
    "    text = [re.sub(r\" co.\", \" company \", sentence) for sentence in text]\n",
    "    text = [re.sub(r\" ltd.\", \" limited \", sentence, flags=re.I) for sentence in text]\n",
    "    text = [re.sub(r\" pvt.\", \" private \", sentence, flags=re.I) for sentence in text]\n",
    "    text = [re.sub(r\" vs\\.?\", \" versus \", sentence, flags=re.I) for sentence in text]\n",
    "    text = [re.sub(r\"ors\\.?\", \"others\", sentence, flags=re.I) for sentence in text]\n",
    "    text2 = []\n",
    "    for index in range(len(text)):\n",
    "        if(index > 0 and text[index] == '' and text[index-1] == ''):\n",
    "            continue\n",
    "        if(index < len(text)-1 and text[index+1] != '' and text[index+1][0] == '\\n' and text[index] == ''):\n",
    "            continue\n",
    "        text2.append(text[index])\n",
    "    text = text2\n",
    "    text = \"\\n\".join(text)\n",
    "    lines = text.split(\"\\n\")\n",
    "    text_new = \" \".join(lines)\n",
    "    text_new = re.sub(\" +\", \" \", text_new)\n",
    "    l_new = []\n",
    "    for token in text_new.split():\n",
    "        if token.lower() not in exclusion_list:\n",
    "            l_new.append(token.strip())\n",
    "    return \" \".join(l_new)\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    regex = re.compile(\"[^a-zA-Z<>.\\s]\")\n",
    "    text_returned = re.sub(regex, \" \", text)\n",
    "    tokens = text_returned.split()\n",
    "    words = []\n",
    "    for word in tokens:\n",
    "        if len(word) > 1 or word in single_words:\n",
    "            words.append(word)\n",
    "    return \" \".join(words)\n",
    "\n",
    "def getSubsFromConjunctions(subs):\n",
    "    moreSubs = []\n",
    "    for sub in subs:\n",
    "        rights = list(sub.rights)\n",
    "        rightDeps = {tok.lower_ for tok in rights}\n",
    "        if \"and\" in rightDeps:\n",
    "            moreSubs.extend([tok for tok in rights if tok.dep_ in SUBJECTS or tok.pos_ == \"NOUN\"])\n",
    "            if len(moreSubs) > 0:\n",
    "                moreSubs.extend(getSubsFromConjunctions(moreSubs))\n",
    "    return moreSubs\n",
    "\n",
    "def getObjsFromConjunctions(objs):\n",
    "    moreObjs = []\n",
    "    for obj in objs:\n",
    "        rights = list(obj.rights)\n",
    "        rightDeps = {tok.lower_ for tok in rights}\n",
    "        if \"and\" in rightDeps:\n",
    "            moreObjs.extend([tok for tok in rights if tok.dep_ in OBJECTS or tok.pos_ == \"NOUN\"])\n",
    "            if len(moreObjs) > 0:\n",
    "                moreObjs.extend(getObjsFromConjunctions(moreObjs))\n",
    "    return moreObjs\n",
    "\n",
    "def getVerbsFromConjunctions(verbs):\n",
    "    moreVerbs = []\n",
    "    for verb in verbs:\n",
    "        rightDeps = {tok.lower_ for tok in verb.rights}\n",
    "        if \"and\" in rightDeps:\n",
    "            moreVerbs.extend([tok for tok in verb.rights if tok.pos_ == \"VERB\"])\n",
    "            if len(moreVerbs) > 0:\n",
    "                moreVerbs.extend(getVerbsFromConjunctions(moreVerbs))\n",
    "    return moreVerbs\n",
    "\n",
    "def findSubs(tok):\n",
    "    head = tok.head\n",
    "    while head.pos_ != \"VERB\" and head.pos_ != \"NOUN\" and head.head != head:\n",
    "        head = head.head\n",
    "    if head.pos_ == \"VERB\":\n",
    "        subs = [tok for tok in head.lefts if tok.dep_ == \"SUB\"]\n",
    "        if len(subs) > 0:\n",
    "            verbNegated = isNegated(head)\n",
    "            subs.extend(getSubsFromConjunctions(subs))\n",
    "            return subs, verbNegated\n",
    "        elif head.head != head:\n",
    "            return findSubs(head)\n",
    "    elif head.pos_ == \"NOUN\":\n",
    "        return [head], isNegated(tok)\n",
    "    return [], False\n",
    "\n",
    "def isNegated(tok):\n",
    "    negations = {\"no\", \"not\", \"n't\", \"never\", \"none\"}\n",
    "    for dep in list(tok.lefts) + list(tok.rights):\n",
    "        if dep.lower_ in negations:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def find_negation(tok):\n",
    "    negations = {\"no\", \"not\", \"n't\", \"never\", \"none\"}\n",
    "    for dep in list(tok.lefts):\n",
    "        if dep.lower_ in negations:\n",
    "            verb = dep.lower_ + \" \" + tok.lemma_\n",
    "            verb_id = [dep.i, tok.i]\n",
    "            return verb, verb_id\n",
    "    verb = tok.lemma_\n",
    "    verb_id = [tok.i]\n",
    "    return verb, verb_id\n",
    "\n",
    "def getObjsFromPrepositions(deps):\n",
    "    objs = []\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"ADP\" and (dep.dep_ == \"prep\" or dep.dep_ == \"agent\"):\n",
    "            for tok in dep.rights:\n",
    "                if (tok.pos_ == \"NOUN\" and tok.dep_ in OBJECTS) or (tok.pos_ == \"PRON\" and tok.lower_ == \"me\"):\n",
    "                    objs.append(tok)\n",
    "                elif tok.dep_ == \"pcomp\":\n",
    "                    for t in tok.rights:\n",
    "                        if (t.pos_ == \"NOUN\" and t.dep_ in OBJECTS) or (t.pos_ == \"PRON\" and t.lower_ == \"me\"):\n",
    "                            objs.append(t)\n",
    "                else:\n",
    "                    objs.extend(getObjsFromPrepositions(tok.rights))\n",
    "    return objs\n",
    "\n",
    "def getAdjectives(toks):\n",
    "    toks_with_adjectives = []\n",
    "    for tok in toks:\n",
    "        adjs = [left for left in tok.lefts if left.dep_ in ADJECTIVES]\n",
    "        adjs.append(tok)\n",
    "        adjs.extend([right for right in tok.rights if tok.dep_ in ADJECTIVES])\n",
    "        tok_with_adj = \" \".join([adj.lower_ for adj in adjs])\n",
    "        toks_with_adjectives.extend(adjs)\n",
    "    return toks_with_adjectives\n",
    "\n",
    "def getObjsFromAttrs(deps):\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"NOUN\" and dep.dep_ == \"attr\":\n",
    "            verbs = [tok for tok in dep.rights if tok.pos_ == \"VERB\"]\n",
    "            if len(verbs) > 0:\n",
    "                for v in verbs:\n",
    "                    rights = list(v.rights)\n",
    "                    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "                    objs.extend(getObjsFromPrepositions(rights))\n",
    "                    if len(objs) > 0:\n",
    "                        return v, objs\n",
    "    return None, None\n",
    "\n",
    "def getObjFromXComp(deps):\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"VERB\" and dep.dep_ == \"xcomp\":\n",
    "            v = dep\n",
    "            rights = list(v.rights)\n",
    "            objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "            objs.extend(getObjsFromPrepositions(rights))\n",
    "            if len(objs) > 0:\n",
    "                return v, objs\n",
    "    return None, None\n",
    "def getAllSubs(v):\n",
    "    verbNegated = isNegated(v)\n",
    "    subs = [tok for tok in v.lefts if tok.dep_ in SUBJECTS and tok.pos_ != \"DET\"]\n",
    "    if len(subs) > 0:\n",
    "        subs.extend(getSubsFromConjunctions(subs))\n",
    "    else:\n",
    "        foundSubs, verbNegated = findSubs(v)\n",
    "        subs.extend(foundSubs)\n",
    "    return subs, verbNegated\n",
    "\n",
    "def getAllObjs(v):\n",
    "    rights = list(v.rights)\n",
    "    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "    objs.extend(getObjsFromPrepositions(rights))\n",
    "    potentialNewVerb, potentialNewObjs = getObjFromXComp(rights)\n",
    "    if (potentialNewVerb is not None and potentialNewObjs is not None and len(potentialNewObjs) > 0):\n",
    "        objs.extend(potentialNewObjs)\n",
    "        v = potentialNewVerb\n",
    "    if len(objs) > 0:\n",
    "        objs.extend(getObjsFromConjunctions(objs))\n",
    "    else:\n",
    "        objs.extend(getObjsFromVerbConj(v))\n",
    "    return v, objs\n",
    "\n",
    "def getAllObjsWithAdjectives(v):\n",
    "    rights = list(v.rights)\n",
    "    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "    if len(objs) == 0:\n",
    "        objs = [tok for tok in rights if tok.dep_ in ADJECTIVES]\n",
    "    objs.extend(getObjsFromPrepositions(rights))\n",
    "    potentialNewVerb, potentialNewObjs = getObjFromXComp(rights)\n",
    "    if (potentialNewVerb is not None and potentialNewObjs is not None and len(potentialNewObjs) > 0):\n",
    "        objs.extend(potentialNewObjs)\n",
    "        v = potentialNewVerb\n",
    "    if len(objs) > 0:\n",
    "        objs.extend(getObjsFromConjunctions(objs))\n",
    "    else:\n",
    "        objs.extend(getObjsFromVerbConj(v))\n",
    "    return v, objs\n",
    "\n",
    "def getObjsFromVerbConj(v):\n",
    "    objs = []\n",
    "    rights = list(v.rights)\n",
    "    for right in rights:\n",
    "        if right.dep_ == \"conj\":\n",
    "            subs, verbNegated = getAllSubs(right)\n",
    "            objs.extend(subs)\n",
    "        else:\n",
    "            objs.extend(getObjsFromVerbConj(right))\n",
    "    return objs\n",
    "\n",
    "def check_tag(compound):\n",
    "    flag = False\n",
    "    res = \"\"\n",
    "    for token in compound:\n",
    "        if token.ent_type_ == \"PERSON\":\n",
    "            flag = True\n",
    "            res = \"<NAME>\"\n",
    "            break\n",
    "        elif token.ent_type_ == \"ORG\":\n",
    "            flag = True\n",
    "            res = \"<ORG>\"\n",
    "            break\n",
    "    return flag, res\n",
    "\n",
    "def generate_compound(token):\n",
    "    token_compunds = []\n",
    "    for tok in token.lefts:\n",
    "        if tok.dep_ in COMPOUNDS:\n",
    "            token_compunds.extend(generate_compound(tok))\n",
    "    token_compunds.append(token)\n",
    "    for tok in token.rights:\n",
    "        if tok.dep_ in COMPOUNDS:\n",
    "            token_compunds.extend(generate_compound(tok))\n",
    "    return token_compunds\n",
    "\n",
    "def generate_verb_advmod(v):\n",
    "    v_compunds = []\n",
    "    for tok in v.lefts:\n",
    "        if tok.dep_ in ADVERBS:\n",
    "            v_compunds.extend(generate_verb_advmod(tok))\n",
    "    v_compunds.append(v)\n",
    "    for tok in v.rights:\n",
    "        if tok.dep_ in ADVERBS:\n",
    "            v_compunds.extend(generate_verb_advmod(tok))\n",
    "    return v_compunds\n",
    "\n",
    "def generate_left_right_adjectives(obj):\n",
    "    obj_desc_tokens = []\n",
    "    for tok in obj.lefts:\n",
    "        if tok.dep_ in ADJECTIVES:\n",
    "            obj_desc_tokens.extend(generate_left_right_adjectives(tok))\n",
    "    obj_desc_tokens.append(obj)\n",
    "    for tok in obj.rights:\n",
    "        if tok.dep_ in ADJECTIVES:\n",
    "            obj_desc_tokens.extend(generate_left_right_adjectives(tok))\n",
    "    return obj_desc_tokens\n",
    "\n",
    "def findSVOs(tokens, len_doc):\n",
    "    svos = []\n",
    "    svo_token_ids = []\n",
    "    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\" and tok.dep_ != \"aux\"]\n",
    "    for v in verbs:\n",
    "        subs, verbNegated = getAllSubs(v)\n",
    "        verb, verb_id = find_negation(v)\n",
    "        if len(subs) > 0:\n",
    "            v, objs = getAllObjs(v)\n",
    "            for sub in subs:\n",
    "                for obj in objs:\n",
    "                    sub_compound = generate_compound(sub)\n",
    "                    obj_compound = generate_compound(obj)\n",
    "                    sub_flag, sub_tag = check_tag(sub_compound)\n",
    "                    obj_flag, obj_tag = check_tag(obj_compound)\n",
    "                    if obj_flag and sub_flag:\n",
    "                        event = (sub_tag, verb, obj_tag)\n",
    "                    elif obj_flag:\n",
    "                        event = (\" \".join(tok.lemma_ for tok in sub_compound), verb, obj_tag)\n",
    "                    elif sub_flag:\n",
    "                        event = (sub_tag, verb, \" \".join(tok.lemma_ for tok in obj_compound))\n",
    "                    else:\n",
    "                        event = (\" \".join(tok.lemma_ for tok in sub_compound), verb, \" \".join(tok.lemma_ for tok in obj_compound))\n",
    "                    svos.append(event)\n",
    "    return svos, svo_token_ids\n",
    "\n",
    "single_words = [\"a\", \"A\", \"<\", \">\", \"i\", \"I\"]\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    regex = re.compile(\"[^a-zA-Z<>.\\s]\")\n",
    "    text_returned = re.sub(regex, \" \", text)\n",
    "    tokens = text_returned.split()\n",
    "    words = []\n",
    "    for word in tokens:\n",
    "        if len(word) > 1 or word in single_words:\n",
    "            words.append(word)\n",
    "    out = \" \".join(words)\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Updated events extraction function for single text processing\n",
    "def extract_events_from_text(content):\n",
    "    content = preprocess(content)\n",
    "    pattern = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s'\n",
    "    content_sents = re.split(pattern, content)\n",
    "    file_svo_text = []\n",
    "    lines = []\n",
    "    for i, line in enumerate(content_sents):\n",
    "        line = line.strip()\n",
    "        lines.append(remove_special_characters(line))\n",
    "    for i, doc in enumerate(nlp.pipe(lines)):\n",
    "        SVO, SVO_Token_IDs = findSVOs(doc, 0)\n",
    "        if len(SVO) > 0:\n",
    "            for eve in SVO:\n",
    "                file_svo_text.append(\" \".join(eve))\n",
    "    # If no event found, fallback: use original string (optional, else return empty string or list)\n",
    "    if not file_svo_text:\n",
    "        return \"\"\n",
    "    return \" ; \".join(file_svo_text)\n",
    "\n",
    "# MAIN SCRIPT\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv = \"filtered_all_removed_conclusion_source.csv\"\n",
    "    output_csv = \"filtered_all_events_removed_conclusion_source.csv\"\n",
    "    \n",
    "    df = pd.read_csv(input_csv)\n",
    "    tqdm.pandas(desc=\"Extracting source events\")\n",
    "    df['source_event'] = df['source_text'].progress_apply(extract_events_from_text)\n",
    "    tqdm.pandas(desc=\"Extracting target events\")\n",
    "    df['target_event'] = df['target_text'].progress_apply(extract_events_from_text)\n",
    "    # Keep rest of columns as they are, but place new columns at the start (optional)\n",
    "    output_cols = ['source_event', 'target_event', 'relation', 'source_type', 'target_type', 'file_name']\n",
    "    # Insert at correct positions, original order with two new columns up front (or edit as you want)\n",
    "    df = df[['source_event', 'target_event', 'relation', 'source_type', 'target_type', 'file_name']]\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Saved extracted events to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c82a56",
   "metadata": {},
   "source": [
    "# updated heuristics events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df8abd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting source events: 100%|██████████| 4664/4664 [00:55<00:00, 84.49it/s]\n",
      "Extracting target events: 100%|██████████| 4664/4664 [00:55<00:00, 84.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved extracted events to filtered_all_updated_events_removed_conclusion_source.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load your spaCy model\n",
    "nlp = spacy.load(\"en_core_web_trf\")  # Or use 'en_core_web_sm' if transformer not installed\n",
    "\n",
    "# Exclusion list for preprocessing\n",
    "alphabet_string = string.ascii_lowercase\n",
    "alphabet_list = list(alphabet_string)\n",
    "exclusion_list = alphabet_list + [\n",
    "    \"no\", \"nos\", \"sub-s\", \"subs\", \"ss\", \"cl\", \"dr\", \"mr\", \"mrs\", \"dr\", \"vs\", \"ch\", \"addl\",\n",
    "]\n",
    "exclusion_list = [word + \".\" for word in exclusion_list]\n",
    "\n",
    "def preprocess(content):\n",
    "    raw_text = re.sub(r\"\\xa0\", \" \", content)\n",
    "    raw_text = raw_text.split(\"\\n\")\n",
    "    text = raw_text.copy()\n",
    "    text = [re.sub(r'[^a-zA-Z0-9.,<>)\\/\\-\\t ]', r'', sentence) for sentence in text]\n",
    "    text = [re.sub(\"\\\\t+\", \" \", sentence) for sentence in text]\n",
    "    text = [re.sub(\"\\\\s+\", \" \", sentence) for sentence in text]\n",
    "    text = [re.sub(\" +\", \" \", sentence) for sentence in text]\n",
    "    text = [re.sub(\"\\\\.\\\\.+\", \"\", sentence) for sentence in text]\n",
    "    text = [re.sub(\"\\\\A ?\", \"\", sentence) for sentence in text]\n",
    "    text = [sentence for sentence in text if (len(sentence) != 1 and not re.fullmatch(\"(\\\\d|\\\\d\\\\d|\\\\d\\\\d\\\\d)\", sentence))]\n",
    "    text = [sentence for sentence in text if len(sentence) != 0]\n",
    "    text = [re.sub('\\\\A\\\\(?([\\\\d\\\\w]{1,3})(\\\\.|\\\\))\\\\s?(?=[A-Z])', '\\\\n', sentence) for sentence in text]\n",
    "    text = [re.sub(\"\\\\A\\\\(([ivx]+)\\\\)\\\\s?(?=[a-zA-Z0-9])\", '\\\\n', sentence) for sentence in text]\n",
    "    text = [re.sub(r\"[()\\\\[\\\\]\\\\\\\"$']\", \" \", sentence) for sentence in text]\n",
    "    text = [re.sub(r\" no.\", \" number \", sentence, flags=re.I) for sentence in text]\n",
    "    text = [re.sub(r\" nos.\", \" numbers \", sentence, flags=re.I) for sentence in text]\n",
    "    text = [re.sub(r\" co.\", \" company \", sentence) for sentence in text]\n",
    "    text = [re.sub(r\" ltd.\", \" limited \", sentence, flags=re.I) for sentence in text]\n",
    "    text = [re.sub(r\" pvt.\", \" private \", sentence, flags=re.I) for sentence in text]\n",
    "    text = [re.sub(r\" vs\\\\.?\", \" versus \", sentence, flags=re.I) for sentence in text]\n",
    "    text = [re.sub(r\"ors\\\\.?\", \"others\", sentence, flags=re.I) for sentence in text]\n",
    "    text2 = []\n",
    "    for index in range(len(text)):\n",
    "        if(index > 0 and text[index] == '' and text[index-1] == ''):\n",
    "            continue\n",
    "        if(index < len(text)-1 and text[index+1] != '' and text[index+1][0] == '\\\\n' and text[index] == ''):\n",
    "            continue\n",
    "        text2.append(text[index])\n",
    "    text = text2\n",
    "    text = \"\\\\n\".join(text)\n",
    "    lines = text.split(\"\\\\n\")\n",
    "    text_new = \" \".join(lines)\n",
    "    text_new = re.sub(\" +\", \" \", text_new)\n",
    "    l_new = []\n",
    "    for token in text_new.split():\n",
    "        if token.lower() not in exclusion_list:\n",
    "            l_new.append(token.strip())\n",
    "    return \" \".join(l_new)\n",
    "\n",
    "# Event extraction helpers (minimal version as above, full pipeline can be added)\n",
    "SUBJECTS = {\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"expl\"}\n",
    "OBJECTS  = {\"dobj\", \"attr\", \"oprd\", \"pobj\"}        # keep\n",
    "PASSIVE_PREP = {\"agent\", \"prep\"}                   # for by-phrases etc.\n",
    "\n",
    "\n",
    "def generate_compound(token):\n",
    "    token_compunds = []\n",
    "    for tok in token.lefts:\n",
    "        if tok.dep_ in COMPOUNDS:\n",
    "            token_compunds.extend(generate_compound(tok))\n",
    "    token_compunds.append(token)\n",
    "    for tok in token.rights:\n",
    "        if tok.dep_ in COMPOUNDS:\n",
    "            token_compunds.extend(generate_compound(tok))\n",
    "    return token_compunds\n",
    "\n",
    "def check_tag(compound):\n",
    "    flag = False\n",
    "    res = \"\"\n",
    "    for token in compound:\n",
    "        if token.ent_type_ == \"PERSON\":\n",
    "            flag = True\n",
    "            res = \"<NAME>\"\n",
    "            break\n",
    "        elif token.ent_type_ == \"ORG\":\n",
    "            flag = True\n",
    "            res = \"<ORG>\"\n",
    "            break\n",
    "    return flag, res\n",
    "\n",
    "def isNegated(tok):\n",
    "    negations = {\"no\", \"not\", \"n't\", \"never\", \"none\"}\n",
    "    for dep in list(tok.lefts) + list(tok.rights):\n",
    "        if dep.lower_ in negations:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def find_negation(tok):\n",
    "    negations = {\"no\", \"not\", \"n't\", \"never\", \"none\"}\n",
    "    for dep in list(tok.lefts):\n",
    "        if dep.lower_ in negations:\n",
    "            verb = dep.lower_ + \" \" + tok.lemma_\n",
    "            return verb\n",
    "    return tok.lemma_\n",
    "\n",
    "def inherit_subject_from_conj(v):\n",
    "    \"\"\"\n",
    "    If v has no nsubj/nsubjpass, walk to its coordinated head\n",
    "    and reuse its subject(s). Handles 'succeed ... and be rejected'.\n",
    "    \"\"\"\n",
    "    subject = []\n",
    "    if v.dep_ == \"conj\" and v.head != v:          # coordinate verb\n",
    "        for tok in v.head.lefts:\n",
    "            if tok.dep_ in SUBJECTS:\n",
    "                subject.append(tok)\n",
    "    return subject\n",
    "\n",
    "def getAllSubs(v):\n",
    "    subs = [tok for tok in v.lefts if tok.dep_ in SUBJECTS and tok.pos_ != \"DET\"]\n",
    "    if not subs:\n",
    "        subs = inherit_subject_from_conj(v)\n",
    "    return subs\n",
    "\n",
    "\n",
    "def getAllObjs(v):\n",
    "    \"\"\"\n",
    "    1. direct objects                VERB -> dobj/attr/oprd\n",
    "    2. prepositional objs            VERB -> prep/agent -> pobj\n",
    "    3. allow zero-object verbs\n",
    "    \"\"\"\n",
    "    rights = list(v.rights)\n",
    "    objs   = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "\n",
    "    # search one hop through prep/agent\n",
    "    for r in rights:\n",
    "        if r.dep_ in PASSIVE_PREP:\n",
    "            objs.extend(t for t in r.rights if t.dep_ == \"pobj\")\n",
    "\n",
    "    return objs                           # may be []\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "def full_verb_phrase(v):\n",
    "    \"\"\"\n",
    "    Combine auxiliaries + negation with the main verb: 'must be rejected'\n",
    "    \"\"\"\n",
    "    auxiliaries = [tok for tok in v.lefts if tok.dep_ in {\"aux\", \"auxpass\"}]\n",
    "    negs        = [tok for tok in v.lefts if tok.dep_ == \"neg\"]\n",
    "    parts = auxiliaries + negs + [v]\n",
    "    return \" \".join(tok.lemma_.lower() for tok in parts)\n",
    "\n",
    "\n",
    "def findSVOs(doc):\n",
    "    events = []\n",
    "    for v in [t for t in doc if t.pos_ == \"VERB\" and t.dep_ != \"aux\"]:\n",
    "        subs = getAllSubs(v)\n",
    "        objs = getAllObjs(v)\n",
    "        verb_phrase = full_verb_phrase(v)\n",
    "\n",
    "        # SV-O\n",
    "        for s in subs:\n",
    "            for o in objs:\n",
    "                events.append((s.lemma_.lower(), verb_phrase, o.lemma_.lower()))\n",
    "\n",
    "        # SV only  (keep even if obj missing)\n",
    "        if subs and not objs:\n",
    "            for s in subs:\n",
    "                events.append((s.lemma_.lower(), verb_phrase, \"\"))\n",
    "\n",
    "        # VO only  (rare in legal text, but keep for completeness)\n",
    "        if objs and not subs:\n",
    "            for o in objs:\n",
    "                events.append((\"\", verb_phrase, o.lemma_.lower()))\n",
    "    return events\n",
    "\n",
    "def extract_events_from_text(text):\n",
    "    preprocessed_text = preprocess(text)\n",
    "    # Simplified pattern to split sentences based on periods and question marks.\n",
    "    # This approach avoids using problematic look-behind assertions.\n",
    "    pattern = r'(?<=[.?!])\\s+'\n",
    "    sentences = re.split(pattern, preprocessed_text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip() != '']\n",
    "    events = []\n",
    "    docs = list(nlp.pipe(sentences))\n",
    "    for doc in docs:\n",
    "        svos = findSVOs(doc)\n",
    "        if svos:\n",
    "            events.extend([\" \".join(event) for event in svos])\n",
    "    # Join events (one string for csv cell)\n",
    "    return \" ; \".join(events) if events else \"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    in_csv = \"filtered_all_removed_conclusion_source.csv\"\n",
    "    out_csv = \"filtered_all_updated_events_removed_conclusion_source.csv\"\n",
    "    df = pd.read_csv(in_csv)\n",
    "    tqdm.pandas(desc=\"Extracting source events\")\n",
    "    df['source_event'] = df['source_text'].progress_apply(extract_events_from_text)\n",
    "    tqdm.pandas(desc=\"Extracting target events\")\n",
    "    df['target_event'] = df['target_text'].progress_apply(extract_events_from_text)\n",
    "    # Save only required columns, in the desired order\n",
    "    final_cols = ['source_event', 'target_event', 'relation', 'source_type', 'target_type', 'file_name']\n",
    "    df = df[final_cols]\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved extracted events to {out_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37189a73",
   "metadata": {},
   "source": [
    "# check how many of them have no events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b89f7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with missing 'source_event': 623\n",
      "Rows with missing 'target_event': 658\n",
      "Rows with missing both 'source_event' and 'target_event': 410\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'filtered_all_updated_events_removed_conclusion_source.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Initialize counters\n",
    "missing_source_event = 0\n",
    "missing_target_event = 0\n",
    "missing_both = 0\n",
    "\n",
    "# Iterate through each row and check for missing values\n",
    "for index, row in df.iterrows():\n",
    "    if pd.isna(row['source_event']) and pd.isna(row['target_event']):\n",
    "        missing_both += 1\n",
    "    elif pd.isna(row['source_event']):\n",
    "        missing_source_event += 1\n",
    "    elif pd.isna(row['target_event']):\n",
    "        missing_target_event += 1\n",
    "\n",
    "# Print the results\n",
    "print(f\"Rows with missing 'source_event': {missing_source_event}\")\n",
    "print(f\"Rows with missing 'target_event': {missing_target_event}\")\n",
    "print(f\"Rows with missing both 'source_event' and 'target_event': {missing_both}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f39ba85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with missing 'source_event' or 'target_event' have been removed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'filtered_all_updated_events_removed_conclusion_source.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop rows where either 'source_event' or 'target_event' is missing\n",
    "filtered_df = df.dropna(subset=['source_event', 'target_event'])\n",
    "\n",
    "# Save the filtered dataframe to a new CSV file\n",
    "filtered_df.to_csv('filtered_all_updated_events_removed_conclusion_source_and_empty_events.csv', index=False)\n",
    "\n",
    "print(\"Rows with missing 'source_event' or 'target_event' have been removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c828ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
