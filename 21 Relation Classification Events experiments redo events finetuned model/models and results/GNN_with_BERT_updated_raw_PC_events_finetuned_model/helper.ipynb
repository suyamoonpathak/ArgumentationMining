{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "294d554b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold comparison analysis...\n",
      "Processed fold_1: Accuracy=0.8774, Macro Precision=0.8504, Macro Recall=0.9262, Macro F1=0.8803\n",
      "Processed fold_2: Accuracy=0.8851, Macro Precision=0.8173, Macro Recall=0.9351, Macro F1=0.8628\n",
      "Processed fold_3: Accuracy=0.8190, Macro Precision=0.7712, Macro Recall=0.8913, Macro F1=0.7960\n",
      "Processed fold_4: Accuracy=0.8184, Macro Precision=0.7820, Macro Recall=0.8834, Macro F1=0.8031\n",
      "Processed fold_5: Accuracy=0.9112, Macro Precision=0.8349, Macro Recall=0.9303, Macro F1=0.8733\n",
      "Processed fold_6: Accuracy=0.8693, Macro Precision=0.7992, Macro Recall=0.8951, Macro F1=0.8289\n",
      "Processed fold_7: Accuracy=0.8812, Macro Precision=0.8300, Macro Recall=0.9348, Macro F1=0.8671\n",
      "Processed fold_8: Accuracy=0.9313, Macro Precision=0.6140, Macro Recall=0.6305, Macro F1=0.6190\n",
      "Processed fold_9: Accuracy=0.8523, Macro Precision=0.8041, Macro Recall=0.9301, Macro F1=0.8456\n",
      "Processed fold_10: Accuracy=0.8919, Macro Precision=0.8410, Macro Recall=0.9261, Macro F1=0.8721\n",
      "Average Macro Precision: 0.7944\n",
      "Average Macro Recall: 0.8883\n",
      "Average Macro F1-Score: 0.8248\n",
      "\n",
      "Average Accuracy: 0.8737\n",
      "\n",
      "Results saved to: fold_comparison_results.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def parse_classification_report(file_path):\n",
    "    \"\"\"Parse classification_report.txt file and extract key metrics\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Extract accuracy\n",
    "    accuracy_match = re.search(r'accuracy\\s+(\\d+\\.\\d+)', content)\n",
    "    accuracy = float(accuracy_match.group(1)) if accuracy_match else None\n",
    "    \n",
    "    # Extract macro avg precision, recall, and f1-score\n",
    "    macro_avg_match = re.search(r'macro avg\\s+(\\d+\\.\\d+)\\s+(\\d+\\.\\d+)\\s+(\\d+\\.\\d+)', content)\n",
    "    macro_precision = float(macro_avg_match.group(1)) if macro_avg_match else None\n",
    "    macro_recall = float(macro_avg_match.group(2)) if macro_avg_match else None\n",
    "    macro_f1 = float(macro_avg_match.group(3)) if macro_avg_match else None\n",
    "    \n",
    "    return accuracy, macro_precision, macro_recall, macro_f1\n",
    "\n",
    "def compare_folds(base_path=\".\", model_name=\"YourModel\"):\n",
    "    \"\"\"Compare results across all folds\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Process each fold\n",
    "    for fold_num in range(1, 11):  # fold_1 to fold_10\n",
    "        fold_dir = Path(base_path) / f\"fold_{fold_num}\"\n",
    "        report_file = fold_dir / \"classification_report.txt\"\n",
    "        \n",
    "        if report_file.exists():\n",
    "            try:\n",
    "                accuracy, macro_precision, macro_recall, macro_f1 = parse_classification_report(report_file)\n",
    "                \n",
    "                results.append({\n",
    "                    'model_name': model_name,\n",
    "                    'fold_number': fold_num,\n",
    "                    'accuracy': accuracy,\n",
    "                    'macro_precision': macro_precision,\n",
    "                    'macro_recall': macro_recall,\n",
    "                    'f1_score_macro_avg': macro_f1\n",
    "                })\n",
    "                \n",
    "                print(f\"Processed fold_{fold_num}: Accuracy={accuracy:.4f}, Macro Precision={macro_precision:.4f}, Macro Recall={macro_recall:.4f}, Macro F1={macro_f1:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing fold_{fold_num}: {e}\")\n",
    "        else:\n",
    "            print(f\"Warning: {report_file} not found\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def generate_comparison_report(results):\n",
    "    \"\"\"Generate comparison report and statistics\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to analyze\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # Calculate averages for all metrics\n",
    "    avg_accuracy = df['accuracy'].mean()\n",
    "    avg_macro_precision = df['macro_precision'].mean()\n",
    "    avg_macro_recall = df['macro_recall'].mean()\n",
    "    avg_macro_f1 = df['f1_score_macro_avg'].mean()\n",
    "\n",
    "    # Add a new row with the average of all metrics\n",
    "    avg_row = pd.DataFrame([{\n",
    "        'model_name': 'Average',\n",
    "        'fold_number': 'Average',\n",
    "        'accuracy': avg_accuracy,\n",
    "        'macro_precision': avg_macro_precision,\n",
    "        'macro_recall': avg_macro_recall,\n",
    "        'f1_score_macro_avg': avg_macro_f1\n",
    "    }])\n",
    "\n",
    "    # Concatenate the average row to the existing dataframe\n",
    "    df = pd.concat([df, avg_row], ignore_index=True)\n",
    "\n",
    "    # Print the averages\n",
    "\n",
    "    print(f\"Average Macro Precision: {avg_macro_precision:.4f}\")\n",
    "    print(f\"Average Macro Recall: {avg_macro_recall:.4f}\")\n",
    "    print(f\"Average Macro F1-Score: {avg_macro_f1:.4f}\")\n",
    "    print(f\"\\nAverage Accuracy: {avg_accuracy:.4f}\")\n",
    "    \n",
    "    # Save results to CSV\n",
    "    output_file = \"fold_comparison_results.csv\"\n",
    "    df.to_csv(output_file, index=False, float_format='%.4f')\n",
    "    print(f\"\\nResults saved to: {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    BASE_PATH = \"results\"  # Current directory, change if needed\n",
    "    MODEL_NAME = \"LEGALBERT_GNN_PROCESSED_UCREAT\"  # Change to your actual model name\n",
    "    \n",
    "    print(\"Starting fold comparison analysis...\")\n",
    "    \n",
    "    # Parse all fold results\n",
    "    results = compare_folds(BASE_PATH, MODEL_NAME)\n",
    "    \n",
    "    if results:\n",
    "        # Generate comparison report\n",
    "        df = generate_comparison_report(results)\n",
    "        \n",
    "        \n",
    "    \n",
    "    else:\n",
    "        print(\"No valid results found. Please check your file paths and formats.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e50648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
