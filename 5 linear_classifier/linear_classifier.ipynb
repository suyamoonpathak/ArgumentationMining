{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1592a375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Found 39 XML files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import umap.umap_ as umap\n",
    "from collections import Counter\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define paths\n",
    "XML_DIR = Path(\"./../3 GNN/xml_files\")\n",
    "MODEL_DIR = Path(\"./../2 second_classifier_premise-vs-conclusion/results/secondAttemptWithMetrics/RoBERTa_prem_conc_finetuned/\")\n",
    "OUTPUT_DIR = Path(\"classifier_results\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load XML files\n",
    "xml_files = [f for f in XML_DIR.glob(\"*.xml\") if f.is_file()]\n",
    "print(f\"Found {len(xml_files)} XML files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fbae17e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_xml(xml_path):\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        nodes = []\n",
    "        id_registry = set()\n",
    "        for elem in root.iter():\n",
    "            if elem.tag not in ('prem', 'conc'):\n",
    "                continue\n",
    "                \n",
    "            node_id = elem.attrib.get('ID', '').strip()\n",
    "            if not node_id or node_id in id_registry:\n",
    "                continue\n",
    "                \n",
    "            nodes.append({\n",
    "                'id': node_id,\n",
    "                'text': elem.text.strip() if elem.text else '',\n",
    "                'type': elem.tag,\n",
    "                'xml_file': xml_path.name\n",
    "            })\n",
    "            id_registry.add(node_id)\n",
    "            \n",
    "        id_to_idx = {node['id']: idx for idx, node in enumerate(nodes)}\n",
    "        \n",
    "        # Extract relations\n",
    "        relations = []\n",
    "        for elem in root.iter():\n",
    "            if elem.tag not in ('prem', 'conc'):\n",
    "                continue\n",
    "                \n",
    "            source_id = elem.attrib.get('ID', '').strip()\n",
    "            if not source_id or source_id not in id_to_idx:\n",
    "                continue\n",
    "\n",
    "            def clean_split(value):\n",
    "                return [t.strip() for t in value.strip().split('|') if t.strip()]\n",
    "                \n",
    "            for rel_type in ['SUP', 'ATT']:\n",
    "                if rel_type in elem.attrib:\n",
    "                    targets = clean_split(elem.attrib[rel_type])\n",
    "                    relation_label = 0 if rel_type == 'SUP' else 1  # 0=Support, 1=Attack\n",
    "                    \n",
    "                    for target_id in targets:\n",
    "                        if target_id in id_to_idx:\n",
    "                            source_idx = id_to_idx[source_id]\n",
    "                            target_idx = id_to_idx[target_id]\n",
    "                            relations.append({\n",
    "                                'source_id': source_id,\n",
    "                                'target_id': target_id,\n",
    "                                'source_idx': source_idx,\n",
    "                                'target_idx': target_idx,\n",
    "                                'label': relation_label,\n",
    "                                'xml_file': xml_path.name\n",
    "                            })\n",
    "        \n",
    "        return nodes, relations, id_to_idx\n",
    "        \n",
    "    except ET.ParseError as e:\n",
    "        print(f\"XML parse error in {xml_path.name}: {e}\")\n",
    "        return [], [], {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "85bd4e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_no_relation_samples(nodes, relations, id_map):\n",
    "    # Create set of existing relations\n",
    "    existing_relations = set()\n",
    "    for rel in relations:\n",
    "        existing_relations.add((rel['source_idx'], rel['target_idx']))\n",
    "    \n",
    "    # Count support and attack relations\n",
    "    support_count = sum(1 for rel in relations if rel['label'] == 0)\n",
    "    attack_count = sum(1 for rel in relations if rel['label'] == 1)\n",
    "    \n",
    "    # Determine sample size based on class frequencies\n",
    "    sample_size = support_count + attack_count * 5  # Prioritize attack relations\n",
    "    \n",
    "    # Generate all possible No-Relation pairs\n",
    "    no_relation_candidates = []\n",
    "    for i, source in enumerate(nodes):\n",
    "        for j, target in enumerate(nodes):\n",
    "            if i != j and (i, j) not in existing_relations:\n",
    "                # Prioritize premise-conclusion pairs as they're more meaningful\n",
    "                priority = 2 if source['type'] == 'prem' and target['type'] == 'conc' else 1\n",
    "                no_relation_candidates.append({\n",
    "                    'source_id': source['id'],\n",
    "                    'target_id': target['id'],\n",
    "                    'source_idx': i,\n",
    "                    'target_idx': j,\n",
    "                    'label': 2,  # 2=No-Relation\n",
    "                    'priority': priority,\n",
    "                    'xml_file': source['xml_file']\n",
    "                })\n",
    "    \n",
    "    # Sort by priority and sample\n",
    "    no_relation_candidates.sort(key=lambda x: x['priority'], reverse=True)\n",
    "    sample_size = min(sample_size, len(no_relation_candidates))\n",
    "    sampled_no_relations = no_relation_candidates[:sample_size]\n",
    "    \n",
    "    print(f\"Sampled {len(sampled_no_relations)} No-Relation pairs from {len(no_relation_candidates)} candidates\")\n",
    "    return sampled_no_relations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a3e41f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../2 second_classifier_premise-vs-conclusion/results/secondAttemptWithMetrics/RoBERTa_prem_conc_finetuned and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RoBERTa model...\n"
     ]
    }
   ],
   "source": [
    "# Load RoBERTa model\n",
    "\n",
    "# PRETRAINED_MODEL_NAME = \"roberta-base\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "# model = AutoModel.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "# model = model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "\n",
    "print(\"Loading RoBERTa model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "model = AutoModel.from_pretrained(MODEL_DIR)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def generate_embeddings(texts, batch_size=8):\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch, \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=512, \n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Get CLS token embeddings\n",
    "        embeddings.append(outputs.last_hidden_state[:,0,:].cpu())\n",
    "    \n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "\n",
    "def generate_embeddings_from_word_embeddings(texts, batch_size=8):\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch, \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=512, \n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        \n",
    "        # Extract token IDs from the tokenizer output\n",
    "        input_ids = inputs['input_ids']\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get the embeddings directly from the embedding layer\n",
    "            word_embeddings = model.embeddings.word_embeddings(input_ids)\n",
    "            \n",
    "            # For each sequence, aggregate word embeddings (e.g., average over all tokens)\n",
    "            # You can also use the embeddings of specific tokens (like [CLS]) if needed\n",
    "            batch_embeddings = word_embeddings.mean(dim=1)  # Mean across all tokens\n",
    "            embeddings.append(batch_embeddings.cpu())\n",
    "    \n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9740332f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing XML files: 100%|██████████| 39/39 [00:00<00:00, 320.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 77 No-Relation pairs from 9043 candidates\n",
      "Sampled 99 No-Relation pairs from 9427 candidates\n",
      "Sampled 56 No-Relation pairs from 2701 candidates\n",
      "Sampled 98 No-Relation pairs from 5328 candidates\n",
      "Sampled 64 No-Relation pairs from 2592 candidates\n",
      "Sampled 114 No-Relation pairs from 9030 candidates\n",
      "Sampled 121 No-Relation pairs from 12323 candidates\n",
      "Sampled 74 No-Relation pairs from 2928 candidates\n",
      "Sampled 65 No-Relation pairs from 5635 candidates\n",
      "Sampled 43 No-Relation pairs from 1293 candidates\n",
      "Sampled 35 No-Relation pairs from 839 candidates\n",
      "Sampled 108 No-Relation pairs from 5786 candidates\n",
      "Sampled 114 No-Relation pairs from 4083 candidates\n",
      "Sampled 75 No-Relation pairs from 3961 candidates\n",
      "Sampled 226 No-Relation pairs from 21324 candidates\n",
      "Sampled 29 No-Relation pairs from 1031 candidates\n",
      "Sampled 77 No-Relation pairs from 2494 candidates\n",
      "Sampled 65 No-Relation pairs from 4627 candidates\n",
      "Sampled 48 No-Relation pairs from 1592 candidates\n",
      "Sampled 38 No-Relation pairs from 1298 candidates\n",
      "Sampled 43 No-Relation pairs from 573 candidates\n",
      "Sampled 79 No-Relation pairs from 3843 candidates\n",
      "Sampled 20 No-Relation pairs from 532 candidates\n",
      "Sampled 67 No-Relation pairs from 6095 candidates\n",
      "Sampled 127 No-Relation pairs from 5194 candidates\n",
      "Sampled 44 No-Relation pairs from 1366 candidates\n",
      "Sampled 105 No-Relation pairs from 12113 candidates\n",
      "Sampled 48 No-Relation pairs from 2604 candidates\n",
      "Sampled 40 No-Relation pairs from 1940 candidates\n",
      "Sampled 48 No-Relation pairs from 1762 candidates\n",
      "Sampled 48 No-Relation pairs from 2320 candidates\n",
      "Sampled 57 No-Relation pairs from 2117 candidates\n",
      "Sampled 56 No-Relation pairs from 2402 candidates\n",
      "Sampled 58 No-Relation pairs from 3030 candidates\n",
      "Sampled 54 No-Relation pairs from 2606 candidates\n",
      "Sampled 224 No-Relation pairs from 21620 candidates\n",
      "Sampled 90 No-Relation pairs from 5022 candidates\n",
      "Sampled 47 No-Relation pairs from 2023 candidates\n",
      "Sampled 71 No-Relation pairs from 4227 candidates\n",
      "Documents with valid pairs: 39\n",
      "Train documents: 31\n",
      "Test documents: 8\n",
      "Train samples: 4162\n",
      "Test samples: 1166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process all XML files first to gather argument pairs\n",
    "all_nodes = []\n",
    "all_relations = []\n",
    "all_no_relations = []\n",
    "doc_to_pairs = {}  # Track pairs by document\n",
    "\n",
    "for xml_file in tqdm(xml_files, desc=\"Processing XML files\"):\n",
    "    nodes, relations, id_map = process_xml(xml_file)\n",
    "    \n",
    "    if not nodes:\n",
    "        print(f\"Skipping {xml_file.name}: No valid nodes found\")\n",
    "        continue\n",
    "    \n",
    "    # Generate no-relation samples\n",
    "    no_relations = generate_no_relation_samples(nodes, relations, id_map)\n",
    "    \n",
    "    # Add to global collections\n",
    "    start_idx = len(all_nodes)\n",
    "    all_nodes.extend(nodes)\n",
    "    all_relations.extend(relations)\n",
    "    all_no_relations.extend(no_relations)\n",
    "    \n",
    "    # Store pairs for this document\n",
    "    doc_pairs = []\n",
    "    \n",
    "    # Extract relation pairs\n",
    "    for rel in relations:\n",
    "        source_node = nodes[rel['source_idx']]\n",
    "        target_node = nodes[rel['target_idx']]\n",
    "        \n",
    "        doc_pairs.append({\n",
    "            'source_text': source_node['text'],\n",
    "            'target_text': target_node['text'],\n",
    "            'label': rel['label'],  # 0=Support, 1=Attack\n",
    "            'document': xml_file.name\n",
    "        })\n",
    "    \n",
    "    # Extract no-relation pairs\n",
    "    for rel in no_relations:\n",
    "        source_node = nodes[rel['source_idx']]\n",
    "        target_node = nodes[rel['target_idx']]\n",
    "        \n",
    "        doc_pairs.append({\n",
    "            'source_text': source_node['text'],\n",
    "            'target_text': target_node['text'],\n",
    "            'label': 2,  # 2=No-Relation\n",
    "            'document': xml_file.name\n",
    "        })\n",
    "    \n",
    "    # Only store if we have pairs for this document\n",
    "    if doc_pairs:\n",
    "        doc_to_pairs[xml_file.name] = doc_pairs\n",
    "\n",
    "# Get documents that have pairs\n",
    "valid_docs = list(doc_to_pairs.keys())\n",
    "print(f\"Documents with valid pairs: {len(valid_docs)}\")\n",
    "\n",
    "# Split documents into train and test\n",
    "train_docs, test_docs = train_test_split(valid_docs, test_size=0.2, random_state=42)\n",
    "print(f\"Train documents: {len(train_docs)}\")\n",
    "print(f\"Test documents: {len(test_docs)}\")\n",
    "\n",
    "# Flatten pairs by document\n",
    "train_pairs = []\n",
    "for doc in train_docs:\n",
    "    train_pairs.extend(doc_to_pairs[doc])\n",
    "\n",
    "test_pairs = []\n",
    "for doc in test_docs:\n",
    "    test_pairs.extend(doc_to_pairs[doc])\n",
    "\n",
    "print(f\"Train samples: {len(train_pairs)}\")\n",
    "print(f\"Test samples: {len(test_pairs)}\")\n",
    "\n",
    "# Verify we have samples in both sets\n",
    "if len(test_pairs) == 0:\n",
    "    print(\"WARNING: No test samples! Using sample-level split instead of document-level split\")\n",
    "    # Fall back to sample-level split\n",
    "    all_pairs = []\n",
    "    for pairs in doc_to_pairs.values():\n",
    "        all_pairs.extend(pairs)\n",
    "    \n",
    "    train_pairs, test_pairs = train_test_split(all_pairs, test_size=0.2, random_state=42)\n",
    "    print(f\"New split - Train: {len(train_pairs)}, Test: {len(test_pairs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "45793aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:48<00:00,  2.68it/s]\n",
      "100%|██████████| 37/37 [00:16<00:00,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings: Train torch.Size([4162, 1536]), Test torch.Size([1166, 1536])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process in batches to avoid memory issues\n",
    "def generate_pair_embeddings(pairs, batch_size=32):\n",
    "    if not pairs:\n",
    "        return None  # Return None for empty sets\n",
    "    \n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(pairs), batch_size)):\n",
    "        batch = pairs[i:i+batch_size]\n",
    "        source_texts = [pair['source_text'] for pair in batch]\n",
    "        target_texts = [pair['target_text'] for pair in batch]\n",
    "        \n",
    "        source_embs = generate_embeddings(source_texts)\n",
    "        target_embs = generate_embeddings(target_texts)\n",
    "        \n",
    "        # Concatenate source and target embeddings\n",
    "        for j in range(len(batch)):\n",
    "            combined = torch.cat([source_embs[j], target_embs[j]], dim=0)\n",
    "            all_embeddings.append(combined)\n",
    "    \n",
    "    return torch.stack(all_embeddings)\n",
    "\n",
    "train_embeddings = generate_pair_embeddings(train_pairs)\n",
    "test_embeddings = generate_pair_embeddings(test_pairs)\n",
    "\n",
    "# Make sure we have embeddings before continuing\n",
    "if train_embeddings is None or test_embeddings is None:\n",
    "    raise ValueError(\"Failed to generate embeddings for both train and test sets\")\n",
    "\n",
    "print(f\"Generated embeddings: Train {train_embeddings.shape}, Test {test_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2ca232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ecc0fd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying UMAP dimensionality reduction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suyamoon/miniconda3/lib/python3.12/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced dimensions: torch.Size([4162, 512])\n"
     ]
    }
   ],
   "source": [
    "# Create a linear layer to increase dimensions to 1024\n",
    "# expand_layer = nn.Linear(768*2, 2048)\n",
    "\n",
    "# Apply linear transformation\n",
    "# with torch.no_grad():\n",
    "    # train_expanded = expand_layer(train_embeddings)\n",
    "    # test_expanded = expand_layer(test_embeddings)\n",
    "\n",
    "\n",
    "print(\"Applying UMAP dimensionality reduction...\")\n",
    "# Use UMAP to reduce to 512 dimensions\n",
    "reducer = umap.UMAP(n_components=512, random_state=42)\n",
    "train_reduced = reducer.fit_transform(train_embeddings.detach().numpy())\n",
    "test_reduced = reducer.transform(test_embeddings.detach().numpy())\n",
    "\n",
    "# Convert back to PyTorch tensors\n",
    "train_reduced = torch.tensor(train_reduced, dtype=torch.float32)\n",
    "test_reduced = torch.tensor(test_reduced, dtype=torch.float32)\n",
    "\n",
    "print(f\"Reduced dimensions: {train_reduced.shape}\")\n",
    "\n",
    "def plot_umap_embeddings(embeddings, labels, title):\n",
    "    \"\"\"Visualize embeddings with class-colored UMAP projection\"\"\"\n",
    "    # reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "    reduced_2d = reducer.fit_transform(embeddings)\n",
    "    \n",
    "    # Convert numeric labels to meaningful names\n",
    "    label_names = ['Support', 'Attack', 'No-Relation']\n",
    "    colors = [label_names[label] for label in labels]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.scatterplot(\n",
    "        x=reduced_2d[:,0], \n",
    "        y=reduced_2d[:,1], \n",
    "        hue=colors,\n",
    "        palette='viridis',\n",
    "        s=50,\n",
    "        alpha=0.7,\n",
    "        edgecolor='none'\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel('UMAP Dimension 1')\n",
    "    plt.ylabel('UMAP Dimension 2')\n",
    "    plt.legend(title='Relationship Type')\n",
    "    plt.savefig(f\"{title.replace(' ', '_').lower()}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Get labels from training pairs\n",
    "train_labels = [pair['label'] for pair in train_pairs]\n",
    "\n",
    "try:\n",
    "    # Plot original RoBERTa embeddings\n",
    "    plot_umap_embeddings(\n",
    "        train_embeddings.numpy(),\n",
    "        train_labels,\n",
    "        \"768d Finetuned RoBERTa Embeddings (embedding layer)\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Plot final UMAP-reduced embeddings\n",
    "    plot_umap_embeddings(\n",
    "        train_reduced.numpy(),\n",
    "        train_labels,\n",
    "        \"512d UMAP-Reduced Embeddings Finetuned Roberta (embedding layer_)\"\n",
    "    )\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"Missing variable: {e}. Run previous processing steps first\")\n",
    "except Exception as e:\n",
    "    print(f\"Visualization error: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b4aa749",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the MLP classifier\n",
    "class RelationClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=512, hidden_dims=[256, 128]):\n",
    "        super(RelationClassifier, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_dim, 3))  # 3 classes: Support, Attack, No-Relation\n",
    "        \n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "def calculate_balanced_class_weights(train_pairs):\n",
    "    # Count instances of each class\n",
    "    support_count = sum(1 for pair in train_pairs if pair['label'] == 0)\n",
    "    attack_count = sum(1 for pair in train_pairs if pair['label'] == 1)\n",
    "    no_relation_count = sum(1 for pair in train_pairs if pair['label'] == 2)\n",
    "    \n",
    "    total_samples = support_count + attack_count + no_relation_count\n",
    "    \n",
    "    # Calculate balanced weights\n",
    "    support_w = total_samples / (3 * support_count)\n",
    "    attack_w = total_samples / (3 * attack_count)\n",
    "    no_rel_w = total_samples / (3 * no_relation_count)\n",
    "    \n",
    "    class_weights = torch.tensor([support_w, attack_w, no_rel_w], dtype=torch.float)\n",
    "    \n",
    "    print(f\"Class weights - Support: {support_w:.2f}, Attack: {attack_w:.2f}, No Relation: {no_rel_w:.2f}\")\n",
    "    return class_weights\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, weight=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        ce_loss = F.cross_entropy(input, target, reduction='none', weight=self.weight)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67199107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset\n",
    "class ArgumentPairDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings.detach().clone()  # Ensure detachment\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'embedding': self.embeddings[idx],\n",
    "            'label': self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_labels = torch.tensor([pair['label'] for pair in train_pairs])\n",
    "test_labels = torch.tensor([pair['label'] for pair in test_pairs])\n",
    "\n",
    "train_dataset = ArgumentPairDataset(train_embeddings, train_labels)\n",
    "test_dataset = ArgumentPairDataset(test_embeddings, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=20):\n",
    "    model = model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            embeddings = batch['embedding'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(embeddings)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            epoch_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'acc': f\"{100 * correct / total:.2f}%\"\n",
    "            })\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                embeddings = batch['embedding'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                outputs = model(embeddings)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"Val Loss: {avg_val_loss:.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), OUTPUT_DIR / \"best_model.pt\")\n",
    "            print(\"Saved new best model\")\n",
    "        \n",
    "        # Generate confusion matrix every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:\n",
    "            cm = confusion_matrix(all_labels, all_preds)\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                       xticklabels=['Support', 'Attack', 'No-Relation'],\n",
    "                       yticklabels=['Support', 'Attack', 'No-Relation'])\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('True')\n",
    "            plt.title(f'Confusion Matrix (Epoch {epoch+1})')\n",
    "            plt.savefig(OUTPUT_DIR / f\"confusion_matrix_epoch_{epoch+1}.png\")\n",
    "            plt.close()\n",
    "    \n",
    "    # Plot loss curves\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(OUTPUT_DIR / \"loss_curves.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fc8131d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution - Support: 1782, Attack: 135, No-Relation: 2457\n"
     ]
    }
   ],
   "source": [
    "label_counts = Counter([pair['label'] for pair in train_pairs])\n",
    "print(f\"Class distribution - Support: {label_counts[0]}, Attack: {label_counts[1]}, No-Relation: {label_counts[2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6ea07c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights - Support: 0.82, Attack: 10.80, No Relation: 0.59\n"
     ]
    }
   ],
   "source": [
    "# Calculate class weights based on training data\n",
    "class_weights = calculate_balanced_class_weights(train_pairs)\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "model = RelationClassifier(input_dim=768*2, hidden_dims=[1024, 512])\n",
    "criterion = FocalLoss(weight=class_weights, gamma=2.0)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d23d623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 137/137 [00:02<00:00, 58.55it/s, loss=0.2002, acc=52.35%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4994, Accuracy: 52.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 34/34 [00:00<00:00, 539.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1524, Accuracy: 64.66%\n",
      "Saved new best model\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 137/137 [00:01<00:00, 73.74it/s, loss=0.2553, acc=52.17%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4020, Accuracy: 52.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 34/34 [00:00<00:00, 560.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1336, Accuracy: 88.31%\n",
      "Saved new best model\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 137/137 [00:01<00:00, 80.16it/s, loss=0.4507, acc=49.82%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3873, Accuracy: 49.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 34/34 [00:00<00:00, 717.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1690, Accuracy: 58.72%\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 137/137 [00:01<00:00, 75.07it/s, loss=0.1008, acc=54.39%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3892, Accuracy: 54.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 34/34 [00:00<00:00, 762.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1543, Accuracy: 83.21%\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 137/137 [00:01<00:00, 75.31it/s, loss=0.0776, acc=49.43%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3857, Accuracy: 49.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 34/34 [00:00<00:00, 816.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1593, Accuracy: 45.73%\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 137/137 [00:01<00:00, 69.33it/s, loss=0.5056, acc=49.82%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3700, Accuracy: 49.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 34/34 [00:00<00:00, 751.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1630, Accuracy: 60.11%\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 137/137 [00:02<00:00, 65.95it/s, loss=0.1260, acc=48.65%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3732, Accuracy: 48.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 34/34 [00:00<00:00, 591.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1499, Accuracy: 58.72%\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 137/137 [00:01<00:00, 69.86it/s, loss=0.2889, acc=45.11%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3713, Accuracy: 45.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 34/34 [00:00<00:00, 776.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.2092, Accuracy: 52.41%\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 137/137 [00:02<00:00, 66.83it/s, loss=0.0891, acc=55.35%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3554, Accuracy: 55.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 34/34 [00:00<00:00, 518.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1909, Accuracy: 58.72%\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 137/137 [00:01<00:00, 68.91it/s, loss=0.5735, acc=45.68%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3661, Accuracy: 45.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 34/34 [00:00<00:00, 659.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.2054, Accuracy: 58.53%\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 137/137 [00:02<00:00, 54.36it/s, loss=0.1251, acc=47.10%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3534, Accuracy: 47.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 34/34 [00:00<00:00, 676.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1678, Accuracy: 14.10%\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 137/137 [00:02<00:00, 53.84it/s, loss=0.0898, acc=46.50%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3480, Accuracy: 46.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 34/34 [00:00<00:00, 482.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1834, Accuracy: 48.89%\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 137/137 [00:02<00:00, 53.11it/s, loss=0.1295, acc=49.36%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3532, Accuracy: 49.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 34/34 [00:00<00:00, 466.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1655, Accuracy: 58.72%\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 137/137 [00:02<00:00, 55.72it/s, loss=0.0880, acc=49.45%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3423, Accuracy: 49.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 34/34 [00:00<00:00, 453.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1451, Accuracy: 79.50%\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 137/137 [00:02<00:00, 53.96it/s, loss=0.8573, acc=55.30%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3474, Accuracy: 55.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 34/34 [00:00<00:00, 470.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1371, Accuracy: 77.27%\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 137/137 [00:02<00:00, 53.62it/s, loss=0.3948, acc=47.42%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3397, Accuracy: 47.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 34/34 [00:00<00:00, 495.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1828, Accuracy: 61.60%\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 137/137 [00:02<00:00, 53.87it/s, loss=0.1710, acc=55.97%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3295, Accuracy: 55.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 34/34 [00:00<00:00, 568.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1270, Accuracy: 95.27%\n",
      "Saved new best model\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 137/137 [00:02<00:00, 53.21it/s, loss=0.8034, acc=59.92%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3077, Accuracy: 59.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 34/34 [00:00<00:00, 649.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1219, Accuracy: 91.93%\n",
      "Saved new best model\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 137/137 [00:02<00:00, 54.38it/s, loss=0.5718, acc=52.08%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3306, Accuracy: 52.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 34/34 [00:00<00:00, 484.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1814, Accuracy: 11.32%\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 137/137 [00:02<00:00, 54.49it/s, loss=0.0634, acc=55.14%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3227, Accuracy: 55.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 34/34 [00:00<00:00, 480.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1388, Accuracy: 56.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final evaluation: 100%|██████████| 34/34 [00:00<00:00, 651.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Support     0.9885    0.8448    0.9110       509\n",
      "      Attack     0.0476    0.4000    0.0851        10\n",
      " No-Relation     0.9964    0.9964    0.9964       559\n",
      "\n",
      "    accuracy                         0.9193      1078\n",
      "   macro avg     0.6775    0.7471    0.6642      1078\n",
      "weighted avg     0.9839    0.9193    0.9476      1078\n",
      "\n",
      "All results saved to classifier_results\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train model\n",
    "print(\"Training model...\")\n",
    "train_losses, val_losses = train_model(\n",
    "    model, train_loader, test_loader, criterion, optimizer, num_epochs=20\n",
    ")\n",
    "\n",
    "# Load best model for final evaluation\n",
    "model.load_state_dict(torch.load(OUTPUT_DIR / \"best_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# Final evaluation\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Final evaluation\"):\n",
    "        embeddings = batch['embedding'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(embeddings)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Generate final confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "           xticklabels=['Support', 'Attack', 'No-Relation'],\n",
    "           yticklabels=['Support', 'Attack', 'No-Relation'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Final Confusion Matrix')\n",
    "plt.savefig(OUTPUT_DIR / \"final_confusion_matrix.png\")\n",
    "plt.close()\n",
    "\n",
    "# Print final classification report\n",
    "report = classification_report(\n",
    "    all_labels, all_preds,\n",
    "    target_names=['Support', 'Attack', 'No-Relation'],\n",
    "    digits=4\n",
    ")\n",
    "print(\"Final Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Save the report\n",
    "with open(OUTPUT_DIR / \"classification_report.txt\", \"w\") as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"All results saved to {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e674743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
