{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63f541ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting fold comparison analysis for folds data...\n",
      "Warning: folds data/test_evaluation/classification_report.txt not found\n",
      "\n",
      "Results saved to: fold_comparison_results.csv\n"
     ]
    }
   ],
   "source": [
    "#folds_comparison\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def parse_classification_report(file_path):\n",
    "    \"\"\"Parse classification_report.txt file and extract key metrics\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Extract accuracy\n",
    "    accuracy_match = re.search(r'accuracy\\s+(\\d+\\.\\d+)', content)\n",
    "    accuracy = float(accuracy_match.group(1)) if accuracy_match else None\n",
    "    \n",
    "    # Extract macro avg precision, recall, and f1-score\n",
    "    macro_avg_match = re.search(r'macro avg\\s+(\\d+\\.\\d+)\\s+(\\d+\\.\\d+)\\s+(\\d+\\.\\d+)', content)\n",
    "    macro_precision = float(macro_avg_match.group(1)) if macro_avg_match else None\n",
    "    macro_recall = float(macro_avg_match.group(2)) if macro_avg_match else None\n",
    "    macro_f1 = float(macro_avg_match.group(3)) if macro_avg_match else None\n",
    "    \n",
    "    return accuracy, macro_precision, macro_recall, macro_f1\n",
    "\n",
    "\n",
    "def compare_folds(base_path=\".\", model_name=\"YourModel\"):\n",
    "    \"\"\"Compare results across all folds\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Process each fold\n",
    "    for fold_num in range(1, 11):  # fold_1 to fold_10\n",
    "        fold_dir = Path(base_path) / f\"fold_{fold_num}\"\n",
    "        report_file = fold_dir / \"classification_report.txt\"\n",
    "        \n",
    "        if report_file.exists():\n",
    "            try:\n",
    "                accuracy, macro_precision, macro_recall, macro_f1 = parse_classification_report(report_file)\n",
    "\n",
    "                # Append fold result to the list\n",
    "                results.append({\n",
    "                    'model_name': model_name,\n",
    "                    'fold_number': fold_num,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision_macro_avg': macro_precision,\n",
    "                    'recall_macro_avg': macro_recall,\n",
    "                    'f1_score_macro_avg': macro_f1\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing fold_{fold_num}: {e}\")\n",
    "        else:\n",
    "            print(f\"Warning: {report_file} not found\")\n",
    "    \n",
    "    # Process test_evaluation folder\n",
    "    test_dir = Path(base_path) / \"test_evaluation\"\n",
    "    test_report_file = test_dir / \"classification_report.txt\"\n",
    "    \n",
    "    if test_report_file.exists():\n",
    "        try:\n",
    "            accuracy, macro_precision, macro_recall, macro_f1 = parse_classification_report(test_report_file)\n",
    "            \n",
    "            results.append({\n",
    "                'model_name': model_name,\n",
    "                'fold_number': 'Test',\n",
    "                'accuracy': accuracy,\n",
    "                'precision_macro_avg': macro_precision,\n",
    "                'recall_macro_avg': macro_recall,\n",
    "                'f1_score_macro_avg': macro_f1\n",
    "            })\n",
    "            \n",
    "            print(f\"Processed test_evaluation: Accuracy={accuracy:.4f}, Precision={macro_precision:.4f}, Recall={macro_recall:.4f}, Macro F1={macro_f1:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing test_evaluation: {e}\")\n",
    "    else:\n",
    "        print(f\"Warning: {test_report_file} not found\")\n",
    "\n",
    "    \n",
    "    return results\n",
    "\n",
    "def generate_comparison_report(results, model_name):\n",
    "    \"\"\"Generate comparison report and statistics\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to analyze\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Separate test results from fold results\n",
    "    fold_results = df[df['fold_number'] != 'Test']\n",
    "    test_results = df[df['fold_number'] == 'Test']\n",
    "    \n",
    "    # Calculate averages and add average row (only from fold results)\n",
    "    if len(fold_results) > 0:\n",
    "        avg_accuracy = fold_results['accuracy'].mean()\n",
    "        avg_precision = fold_results['precision_macro_avg'].mean()\n",
    "        avg_recall = fold_results['recall_macro_avg'].mean()\n",
    "        avg_f1_macro = fold_results['f1_score_macro_avg'].mean()\n",
    "        \n",
    "        avg_row = {\n",
    "            'model_name': model_name,\n",
    "            'fold_number': 'Avg',\n",
    "            'accuracy': avg_accuracy,\n",
    "            'precision_macro_avg': avg_precision,\n",
    "            'recall_macro_avg': avg_recall,\n",
    "            'f1_score_macro_avg': avg_f1_macro\n",
    "        }\n",
    "        \n",
    "        # Reconstruct dataframe: folds + avg + test\n",
    "        df = pd.concat([\n",
    "            fold_results, \n",
    "            pd.DataFrame([avg_row]), \n",
    "            test_results\n",
    "        ], ignore_index=True)\n",
    "\n",
    "    # Sort the dataframe for clarity\n",
    "    df.sort_values(by=['model_name', 'fold_number'], ascending=[True, True], inplace=True)\n",
    "\n",
    "    # Save results to CSV\n",
    "    output_file = \"fold_comparison_results.csv\"\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if not os.path.exists(output_file):\n",
    "        # If file doesn't exist, write the header\n",
    "        df.to_csv(output_file, index=False, float_format='%.4f')\n",
    "        print(f\"\\nResults saved to: {output_file}\")\n",
    "    else:\n",
    "        # If file exists, append without writing the header\n",
    "        df.to_csv(output_file, index=False, header=False, mode='a', float_format='%.4f')\n",
    "        print(f\"\\nResults appended to: {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    models = [\"folds data\"]  # List of models to process\n",
    "    \n",
    "    for model_name in models:\n",
    "        BASE_PATH = f\"./{model_name}\"  # Adjust the path to the model folder\n",
    "\n",
    "        print(f\"\\nStarting fold comparison analysis for {model_name}...\")\n",
    "        \n",
    "        # Parse all fold results\n",
    "        results = compare_folds(BASE_PATH, model_name)\n",
    "        \n",
    "        if results:\n",
    "            # Generate comparison report\n",
    "            df = generate_comparison_report(results, model_name)\n",
    "\n",
    "        else:\n",
    "            print(f\"No valid results found for {model_name}. Please check your file paths and formats.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28367703",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "argmining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
